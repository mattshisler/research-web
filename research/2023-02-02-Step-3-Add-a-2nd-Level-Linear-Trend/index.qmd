---
title: "Step 3 - Add a Second Level Linear Trend"
description: "In this extension we add a linear trend to the second level of the hierarchical model and re-derive the full conditionals."
author:
  - name: Matthew Shisler
    affiliation: North Carloina State University - Department of Statistics
    affiliation-url: https://statistics.sciences.ncsu.edu/ 
categories: [Bayesian, MCMC, Hierarchical Model] # self-defined categories
draft: false 
format:
  html: 
    code-fold: true
execute: 
  cache: true
  freeze: auto
---


```{r}
#| label: load-packages
#| output: false
#| code-summary: "Code: Load the packages"

library(tictoc)
library(Rfast)
```

Briefly restating notation from earlier, $\mathbf{Y}_i$ is an $n_i \times 1$ response vector , $\mathbf{X}$ an $m \times p$ parent design matrix, and $\boldsymbol\delta_i$ a $p \times 1$ parameter vector corresponding to subject $i = 1,\dots,N$. The matrix $\widetilde{\mathbf{X}}_i$ is an $n_i \times p$ matrix associated with subject $i$ which is construct with a sample of rows from the parent design matrix $\mathbf{X}$. The entries of $\mathbf{Y}_i$ are mutually independent with constant variance $\sigma^2$, $\text{Cov}(\mathbf{Y}_i) = \sigma^2 \mathbf{I}_{n_i}$ for all $i$. Further, $\mathbf{Y}_1,\dots,\mathbf{Y}_N$ are mutually independent.

In this step we will add a linear trend to the mean of the distribution for $\boldsymbol\delta_i$. There will be more parameters to estimate and some modification to the full conditionals for the Gibbs sampler. Let $\mathbf{z}_i$ be a $q \times 1$ vector of covariates, including an intercept element in the first position.

The linear trend in the random effects distribution for $\boldsymbol\delta_i$ is represented as a linear combination $\sum_{l=1}^q\boldsymbol\beta_l z_{il}$ where $\boldsymbol\beta_l$ are $p \times 1$ vectors. There are two ways to write this expression in matrix notation. Let $\boldsymbol\beta = (\boldsymbol\beta_1^T, \boldsymbol\beta_2^T, \dots, \boldsymbol\beta_q^T)^T$ be the $qp \times 1$ vector of stacked $\boldsymbol\beta_i$ vectors and let $\mathbf{B} = [\boldsymbol\beta_1 \;\;\; \boldsymbol\beta_2 \;\;\; \dots \;\;\; \boldsymbol\beta_q]$ be the $p \times q$ matrix of arranged $\boldsymbol\beta_i$ vectors. Note, $\boldsymbol\beta = \text{vec}(\mathbf{B})$.  Either we write

\begin{equation}
\tag{1}
\boldsymbol{\mathcal{Z}}_i\boldsymbol\beta = 
\begin{bmatrix}
1 & \dots & 0 & z_{i2} & \dots & 0 & & z_{iq} & \dots & 0\\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \dots & \vdots & \ddots & \vdots\\
0 & \dots & 1 & 0 & \dots & z_{i2} & & 0 & \dots & z_{iq}
\end{bmatrix}_{p \, \times \, qp}
\begin{bmatrix}
\boldsymbol\beta_1\\
\boldsymbol\beta_2\\
\vdots\\
\boldsymbol\beta_q
\end{bmatrix}_{qp \, \times \, 1}
\end{equation}
or we could write
\begin{equation}
\tag{2}
\mathbf{B}\mathbf{z}_i =
\begin{bmatrix}
\boldsymbol\beta_1 & \boldsymbol{\beta}_2 & \dots & \boldsymbol\beta_q
\end{bmatrix}_{p \,\times \, q}
\begin{bmatrix}
1\\
z_{i2}\\
\vdots\\
z_{iq}
\end{bmatrix}_{q \, \times \, 1}
\end{equation}

The former is attractive for deriving analytical expressions and the latter is attractive for some computational advantages. The model is defined to be as follows,

\begin{align*}
\mathbf{Y}_i &\sim \text{Normal}_{n_i}\left(\widetilde{\mathbf{X}}_i\boldsymbol\delta_i, \; \sigma^2 \mathbf{I}_{n_i}\right)\\
\boldsymbol\delta_i &\sim \text{Normal}_p\left(\boldsymbol{\mathcal{Z}}_i\boldsymbol\beta, \; \mathbf{\Omega}\right)
\end{align*}

Again, we will assume $\mathbf{\Omega}$ is diagonal and let $\omega_{kk}$ be the $k$th diagonal element. Next we specify priors,
\begin{align*}
\boldsymbol\beta &\sim \text{Normal}_{qp}\left(\boldsymbol\mu, \; \mathbf{\Lambda}\right)\\
\omega_{kk} &\sim \text{InvGamma}\left(a_\omega, \; b_\omega \right)\\
\sigma^2 &\sim \text{InvGamma}\left(a_\sigma, \; b_\sigma\right)
\end{align*}

The full conditionals in this model are as follows,

\begin{align*}
\boldsymbol\delta_i|\text{ rest} &\sim \text{Normal}_p(\mathbf{V}_i^{-1}\mathbf{M}_i, \mathbf{V}_i^{-1})\\
\mathbf{V}_i &= \frac{1}{\sigma^2} \widetilde{\mathbf{X}}_i^T\widetilde{\mathbf{X}}_i + \mathbf{\Omega}^{-1}\\
\mathbf{M}_i &= \frac{1}{\sigma^2} \widetilde{\mathbf{X}}_i^T\mathbf{Y}_i + \mathbf{\Omega}^{-1}\boldsymbol{\mathcal{Z}}_i\boldsymbol\beta\\\\
\boldsymbol\beta|\text{ rest} &\sim \text{Normal}_p(\mathbf{V}_\beta^{-1}\mathbf{M}_\beta, \mathbf{V}_\beta^{-1})\\
\mathbf{V}_\beta &= \sum_{i=1}^N\boldsymbol{\mathcal{Z}}^T_i\mathbf{\Omega}^{-1}\boldsymbol{\mathcal{Z}}_i + \mathbf{\Lambda}^{-1}\\
\mathbf{M}_\beta &= \sum_{i=1}^N\boldsymbol{\mathcal{Z}}^T_i\mathbf{\Omega}^{-1}\boldsymbol\delta_i + \mathbf{\Lambda}^{-1}\boldsymbol\mu\\\\
\omega_{kk}|\text{ rest} &\sim \text{InvGamma}(A_\omega,B_\omega)\\
A_\omega &= N/2 + a_\omega\\
B_\omega &= \frac{1}{2}\sum_{i=1}^N (\delta_{ik} - (\boldsymbol{\mathcal{Z}}_i\boldsymbol\beta)_k)^2 + b_\omega\\\\
\sigma^2|\text{ rest} &\sim \text{InvGamma}(A_\sigma,B_\sigma)\\
A_\sigma &= \frac{1}{2}\sum_{i=1}^N n_i + a_\sigma\\
B_\sigma &= \frac{1}{2}\sum_{i=1}^N (\mathbf{Y}_i - \widetilde{\mathbf{X}}_i\boldsymbol\delta_i)^T(\mathbf{Y}_i - \widetilde{\mathbf{X}}_i\boldsymbol\delta_i) + b_\sigma
\end{align*}

A few computational remarks, in the full conditional for $\boldsymbol\beta$, we have the expression $\sum_{i=1}^N\boldsymbol{\mathcal{Z}}^T_i\boldsymbol\Omega^{-1}\boldsymbol{\mathcal{Z}}_i$ which would be much too naive to compute directly for each MCMC iteration. Instead, we will rewrite this expression in such a way that will allow us to simplify computations inside the Gibbs loop. First, consider a different and perhaps more natural organization of the covariates $\mathbf{z}_i$, into a matrix $\mathbf{Z} = (\mathbf{z}_1^T, \dots,\mathbf{z}_N^T)^T$
\begin{equation}
\tag{3}
\mathbf{Z} = 
\begin{bmatrix}
1 & z_{12} & \dots & z_{1q}\\
\vdots & & \vdots &\\
1 & z_{N2} & \dots & z_{Nq}
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{z}_1^T\\
\vdots\\
\mathbf{z}_N^T\\
\end{bmatrix}
\end{equation}

Let $\otimes$ represent the Kronecker product. The following identity holds,
\begin{equation}
\tag{4}
\sum_{i=1}^N\boldsymbol{\mathcal{Z}}^T_i\boldsymbol\Omega^{-1}\boldsymbol{\mathcal{Z}}_i = \mathbf{Z}^T\mathbf{Z} \otimes \boldsymbol\Omega^{-1}
\end{equation}

With $\mathbf{Z}$ known we can compute $\mathbf{Z}^T\mathbf{Z}$ outside of the Gibbs loop. 

Also in the full conditional for $\boldsymbol\beta$ we have the expression $ \sum_{i=1}^N\boldsymbol{\mathcal{Z}}^T_i\mathbf{\Omega}^{-1}\boldsymbol\delta_i$. Unfortunately, with $\boldsymbol\delta_i$ and $\boldsymbol\Omega^{-1}$ being parameters to update, this expression must be fully evaluated at each MCMC iteration. Let $\boldsymbol\Delta = (\boldsymbol\delta_1,\dots,\boldsymbol\delta_N)$ be the $p \times N$ matrix of arranged parameter vectors, $\boldsymbol\delta_i$, and $\mathbf{Z}$ be defined as before in $(3)$. A useful identity in computing the quantity of interest is
\begin{equation}
\tag{5}
\sum_{i=1}^N\boldsymbol{\mathcal{Z}}^T_i\mathbf{\Omega}^{-1}\boldsymbol\delta_i =  \text{vec}(\Omega^{-1}\boldsymbol\Delta\mathbf{Z}) = (\mathbf{Z}^T \otimes \, \boldsymbol\Omega^{-1})\text{vec}(\boldsymbol\Delta).
\end{equation}

For the other full conditionals in which $\boldsymbol{\mathcal{Z}}_i\boldsymbol\beta$ appears, namely $\boldsymbol\delta_i$ and $\omega_{kk}$, we can replace with appropriate and readily computed form $\mathbf{B}\mathbf{z}_i$ as defined in $(2)$.

Simulate some data from this model. In this case will we set $p = 2$, $q = 2$, $N = 100$, $n_i = n = 100$, and $m = 400$. 
```{r}
#| code-summary: "Simulate the data"

# dimensions
N <- 100
m <- 400
n <- rep(100, N)
p <- 2
q <- 2

# Design matrices
Xp <- matrix(c(rep(1,m), rnorm(m*(p-1), mean = 0, sd = 5)), nrow = m, ncol = p)
Z  <- matrix(c(rep(1,N), rnorm(N*(q-1), mean = 0, sd = 5)), nrow = N, ncol = q)

# beta parameters
B0     <- Rfast::rmvnorm(q, rep(0,p), (5^2)*diag(2))
beta0  <- matrix(c(B0), ncol = 1)
Omega0 <- diag(c(2,1))

# delta parameters
delta0  <- matrix(0, nrow = p, ncol = N)
sigma20 <- 1

# sample data
Y <- list()
X <- list()
for (i in 1:N){
  # draw delta
  delta0[,i]   <- t(Rfast::rmvnorm(1, B0%*%Z[i,], Omega0))
  
  # draw rows from parent X
  subject_rows <- sample(1:m, n[i])
  X[[i]] <- Xp[subject_rows,]
  
  # draw response
  Y[[i]] <- matrix(rnorm(n[i], mean = X[[i]]%*%delta0[,i], sd = sqrt(sigma20)), ncol=1)
}
```

```{r}
#| code-summary: "Run the Gibbs Sampler"

# set-up
niter <- 5000
keep_delta  <- array(NA, dim = c(p, N, niter))
keep_B      <- array(NA, dim = c(p, q, niter))
keep_Omega  <- matrix(NA, nrow = niter, ncol = p)
keep_sigma2 <- rep(NA, niter)

# initial values
delta  <- matrix(0, nrow = p, ncol = N)
beta   <- rep(10, q*p)
B      <- matrix(c(beta), nrow=p)
sigma2 <- 3
Omega  <- diag(c(5,5))
keep_delta[,,1] <- delta
keep_B[,,1]     <- B
keep_Omega[1,]  <- diag(Omega)
keep_sigma2[1]  <- sigma2

# prior parameters
mu    <- rep(0, q*p)
Lambda_inv <- diag(rep(1e-06,q*p))
a     <- 0.1
b     <- 0.1

# pre-computes
XtX <- list()
XtY <- list()
for (k in 1:N){
  XtX[[k]] <- t(X[[k]])%*%X[[k]]
  XtY[[k]] <- t(X[[k]])%*%Y[[k]]
}
ZtZ <- t(Z)%*%Z
Lmu <- Lambda_inv%*%mu
Ao    <- N/2 + a
As    <- sum(n)/2 + a

tic()
# Gibbs Loop
for (iter in 2:niter){

  Omega_inv <- diag(1/diag(Omega))
  
  # sample deltas
  for (i in 1:N){
    M         <- (1/sigma2)*XtY[[i]] + Omega_inv%*%B%*%Z[i,]
    V_inv     <- chol2inv(chol((1/sigma2)*XtX[[i]] + Omega_inv))
    delta[,i] <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)
  }
  
  # sample beta
  M     <- kronecker(t(Z), Omega_inv)%*%matrix(c(delta), ncol = 1) + Lmu
  V_inv <- solve(kronecker(ZtZ, Omega_inv) + Lambda_inv)
  beta  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(q*p)
  B     <- matrix(beta, nrow = p)
  
  # sample omegas
  for (k in 1:p){
    Bo <- sum((delta[k,] - (B%*%t(Z))[k,])^2)/2 + b
    Omega[k,k] <- 1/rgamma(1, Ao, Bo)
  }
  
  # sample sigma2
  SSE <- 0
  for (i in 1:N){
    SSE <- SSE + sum((Y[[i]] - X[[i]]%*%delta[,i])^2)
  }
  Bs <- SSE/2 + b
  sigma2 <- 1/rgamma(1, As, Bs)
  
  # store everything
  keep_delta[,,iter] <- delta
  keep_B[,,iter]     <- B
  keep_Omega[iter,]  <- diag(Omega)
  keep_sigma2[iter]  <- sigma2
}
toc()


```

Now for some trace plots. The following are for $\mathbf{B}$, $\boldsymbol\Omega$, and $\sigma^2$. And display iterations 100:5000. This visual inspection seems to indicate good convergence!
```{r}
#| code-summary: "Construct Trace Plots"

win <- 100:niter

par(mfrow = c(2,2))

for(l in 1:q){
  for (k in 1:p){
    plot(win, keep_B[k, l, win], type = "l",
         ylab = bquote(beta[paste(.(l),",",.(k))]),
         xlab = "iter")
    abline(h = B0[k, l], col = "red")
  }
}

for (k in 1:p){
  plot(win, keep_Omega[win, k], type = "l",
       ylab = bquote(omega[paste(.(k),",",.(k))]),
       xlab = "iter")
  abline(h = Omega[k, k], col = "red")
}

plot(win, keep_sigma2[win], type = "l",
     ylab = bquote(sigma^2),
     xlab = "iter")
abline(h = sigma20, col = "red")

```