[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew Shisler",
    "section": "",
    "text": "Matthew Shisler is a statistics Ph.D. student at North Carolina State University in Raleigh, NC.\nHe is a military officer serving as an Operations Research Analyst for the U.S. Air Force and an Admissions Liaison Officer for the U.S. Air Force Academy."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Matthew Shisler",
    "section": "Education",
    "text": "Education\n North Carolina State University | Raleigh, NC\nPh.D. in Statistics | in progress\n Purdue University | West Lafayette, IN\nM.S. in Industrial Engineering | 2019\n U.S. Air Force Academy | CO Springs, CO\nB.S. in Applied Mathematics | 2017"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Matthew Shisler",
    "section": "Experience",
    "text": "Experience\nUnited States Air Force | various locations\nOperations Research Analyst | 2017 - present\nUnited States Air Force | various locations\nU.S. Air Force Academy Admissions Officer | 2021 - present"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/2023-02-02-Step-0-Linear-Regression-Gibbs-Sampling/index.html",
    "href": "posts/2023-02-02-Step-0-Linear-Regression-Gibbs-Sampling/index.html",
    "title": "Step 0 - Bayesian Linear Regression with Gibbs Sampling",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\n\n\nWe will start by verifying that our Gibbs sampler is actually working for a very simple case. First, we will simulate data from the model\n\\[\\begin{align*}\n\\mathbf{Y} \\sim \\text{Normal}_n(\\mathbf{X}\\mathbf{\\boldsymbol\\delta}, \\; \\sigma^2 \\mathbf{I}_n)\n\\end{align*}\\]\nwhere \\(\\mathbf{Y}\\) is an \\(n \\times 1\\) response vector, \\(\\mathbf{X}\\) is an \\(n \\times p\\) design matrix, and \\(\\boldsymbol\\delta\\) is a \\(p \\times 1\\) vector of parameters. The observations are mutually independent with constant variance, \\(\\text{Cov}(\\mathbf{Y}) = \\sigma^2 \\mathbf{I}_n\\).\nNext specify priors for \\(\\boldsymbol\\delta\\) and \\(\\sigma^2\\), chosen for conjugacy, \\[\\begin{align*}\n\\boldsymbol\\delta &\\sim \\text{Normal}_p(\\boldsymbol\\beta, \\; \\mathbf{\\Omega})\\\\\n\\sigma^2 &\\sim \\text{InvGamma}(a, \\; b)\n\\end{align*}\\]\nWhere \\(\\boldsymbol\\beta = \\boldsymbol0\\), \\(\\mathbf{\\Omega} = \\text{diag}((1000^2, 1000^2))\\), and \\(a = b = 0.1\\). The code alternates between sampling from the full conditionals, \\(p(\\boldsymbol\\delta|\\mathbf{Y},\\sigma^2)\\) and \\(p(\\sigma^2|\\mathbf{Y},\\boldsymbol\\delta)\\). In this case \\[\\begin{align*}\n  \\boldsymbol\\delta|\\mathbf{Y},\\sigma^2 &\\sim \\text{Normal}\\left(V^{-1}M, V^{-1}\\right)\\\\\n  V &= \\frac{1}{\\sigma^{2}}\\mathbf{X}^T\\mathbf{X} + \\mathbf{\\Omega}^{-1}\\\\\n  M &= \\frac{1}{\\sigma^{2}}\\mathbf{X}^T\\mathbf{Y} + \\mathbf{\\Omega}^{-1}\\boldsymbol\\beta\\\\\n  \\sigma^2|\\mathbf{Y},\\boldsymbol\\delta &\\sim \\text{InvGamma}\\left(\\frac{n}{2} + a, \\frac{1}{2}(\\mathbf{Y}-\\mathbf{X}\\boldsymbol\\delta)^T(\\mathbf{Y}-\\mathbf{X}\\boldsymbol\\delta) + b \\right)\n\\end{align*}\\]\nLet’s do a simple example with \\(n=100\\) and \\(p = 2\\). I will start the parameter index at \\(1\\), i.e. \\(\\delta_1\\) is the intercept.\n\n\nSimulate some data\nn  <- 100\np  <- 2    #including intercept\n\n# Generate some fake data (with intercept)\nX      <- matrix(c(rep(1,n), rnorm(n*(p-1))), nrow = n, ncol = p)\ndelta0 <- rnorm(p, mean = 0, sd = 3)\nY      <- matrix(rnorm(n, X%*%delta0, sd = 1))\n\n\n\n\nRun the Gibbs sampler\n# set-up\nniter <- 5000\nkeep_delta  <- matrix(NA, nrow = niter, ncol = p)\nkeep_sigma2 <- rep(NA,niter)\n\n# initial values (chosen to be intentionally poor)\ndelta  <- c(-10,10)\nsigma2 <- 10\nkeep_delta[1,] <- delta\nkeep_sigma2[1] <- sigma2\n\n# prior parameters\na  <- 0.1\nb  <- 0.1\nbeta <- rep(0,p)\nOmega_inv <- diag(rep(1e-06,p))\n\n# pre-computes\nXtY <- t(X)%*%Y\nXtX <- t(X)%*%X\nObeta <- Omega_inv%*%beta # not really necessary since beta = 0\n\n# Gibbs Loop\ntic()\nfor (i in 2:niter){\n  \n  # Sample from delta full conditional\n  M     <- (1/sigma2)*XtY + Obeta\n  V_inv <- solve((1/sigma2)*XtX + Omega_inv)\n  delta <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n    \n  # Sample from sigma2 full conditional\n  A      <- n/2 + a\n  B      <- (1/2)*sum((Y-X%*%delta)^2) + b\n  sigma2 <- 1/rgamma(1, A, B)\n  \n  # store the results\n  keep_delta[i,]  <- delta\n  keep_sigma2[i] <- sigma2\n  \n}\ntoc()\n\n\n0.35 sec elapsed\n\n\nLet’s inspect the resulting trace plots as a quick visual check. The true value of the respective parameter is represented by a red horizontal line. To make the convergence more obvious, I’ve included the poor initial guess.\n\n\nCode: Generate trace plots\nwin <- 1:niter\n\npar(mfrow = c(2,2))\n\nplot(win, keep_delta[win,1], type = \"l\",\n     ylab = expression(delta[1]),\n     xlab = \"iter\")\nabline(h = delta0[1], col = \"red\")\n\nplot(win, keep_delta[win,2], type = \"l\",\n     ylab = expression(delta[2]),\n     xlab = \"iter\")\nabline(h = delta0[2], col = \"red\")\n\nplot(win, keep_sigma2[win],   type = \"l\",\n     ylab = expression(sigma^2),\n     xlab = \"iter\")\nabline(h = 1, col = \"red\")\n\n\n\n\n\nWe can “burn” the first 100 iterations to see the behavior after convergence. Looks good to me!\n\n\nCode: Generate trace plots\nwin <- 100:niter\n\npar(mfrow = c(2,2))\n\nplot(win, keep_delta[win,1], type = \"l\",\n     ylab = expression(delta[1]),\n     xlab = \"iter\")\nabline(h = delta0[1], col = \"red\")\n\nplot(win, keep_delta[win,2], type = \"l\",\n     ylab = expression(delta[2]),\n     xlab = \"iter\")\nabline(h = delta0[2], col = \"red\")\n\nplot(win, keep_sigma2[win],   type = \"l\",\n     ylab = expression(sigma^2),\n     xlab = \"iter\")\nabline(h = 1, col = \"red\")"
  },
  {
    "objectID": "posts/2023-02-02-Step-1-Hierarchical-Linear-Regression/index.html",
    "href": "posts/2023-02-02-Step-1-Hierarchical-Linear-Regression/index.html",
    "title": "Step 1 - Bayesian Hierarchical Linear Regression",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\nlibrary(Rfast)\n\n\nConsider an extension of the setting from Step 1 where \\(\\mathbf{Y}_i\\) is an \\(n_i \\times 1\\) response vector , \\(\\mathbf{X}_i\\) an \\(n_i \\times p\\) design matrix, and \\(\\boldsymbol\\delta_i\\) a \\(p \\times 1\\) parameter vector corresponding to subject \\(i = 1,\\dots,N\\) (where the subject will later be the year). I did not index year using “\\(t\\)” because later we will define \\(t_{ij}\\) to be the day of year \\(i\\) on which the \\(j\\)th measurement of \\(\\mathbf{Y}_i\\) was collected.\nThe parameters \\(\\boldsymbol\\delta_i\\) are drawn from a multivariate normal random effects distribution with mean \\(\\beta\\) and covariance matrix \\(\\mathbf{\\Omega}\\). The entries of \\(\\mathbf{Y}_i\\) are mutually independent with constant variance \\(\\sigma^2\\), \\(\\text{Cov}(\\mathbf{Y}_i) = \\sigma^2 \\mathbf{I}_n\\) for all \\(i\\). Further, \\(\\mathbf{Y}_1,\\dots,\\mathbf{Y}_N\\) are mutually independent.\n\\[\\begin{align*}\n\\mathbf{Y}_i &\\sim \\text{Normal}_{n_i}\\left(\\mathbf{X}_i\\boldsymbol\\delta_i, \\; \\sigma^2 \\mathbf{I}_{n_i}\\right)\\\\\n\\boldsymbol\\delta_i &\\sim \\text{Normal}_p\\left(\\boldsymbol\\beta, \\; \\mathbf{\\Omega}\\right)\n\\end{align*}\\]\nIn this case we will assume \\(\\mathbf{\\Omega}\\) is diagonal and let \\(\\omega_{kk}\\) be the \\(k\\)th diagonal element. Next specify priors, \\[\\begin{align*}\n\\boldsymbol\\beta &\\sim \\text{Normal}_p\\left(\\boldsymbol\\mu, \\; \\mathbf{\\Lambda}\\right)\\\\\n\\omega_{kk} &\\sim \\text{InvGamma}\\left(a_{k}, \\; b_{k} \\right)\\\\\n\\sigma^2 &\\sim \\text{InvGamma}\\left(a, \\; b\\right)\n\\end{align*}\\]\nNote, for simplicity there is no linear trend in the random effects distribution for \\(\\boldsymbol\\delta_i\\). Also, I’ve left its covariance to be diagonal, just to avoid the Inverse Wishart prior for now. This way we can update the diagonal elements of \\(\\mathbf{\\Omega}\\) individually.\nThe full conditionals in this model are. . .\n\\[\\begin{align*}\n\\boldsymbol\\delta_i|\\text{``rest\"} &\\sim \\text{Normal}_p(\\mathbf{V}_i^{-1}\\mathbf{M}_i, \\mathbf{V}_i^{-1})\\\\\n\\mathbf{V}_i &= \\frac{1}{\\sigma^2} \\mathbf{X}_i^T\\mathbf{X}_i + \\mathbf{\\Omega}^{-1}\\\\\n\\mathbf{M}_i &= \\frac{1}{\\sigma^2} \\mathbf{X}_i^T\\mathbf{Y}_i + \\mathbf{\\Omega}^{-1}\\boldsymbol\\beta\\\\\\\\\n\\boldsymbol\\beta|\\text{``rest\"} &\\sim \\text{Normal}_p(\\mathbf{V}_\\beta^{-1}\\mathbf{M}_\\beta, \\mathbf{V}_\\beta^{-1})\\\\\n\\mathbf{V}_\\beta &= N\\mathbf{\\Omega}^{-1} + \\mathbf{\\Lambda}^{-1}\\\\\n\\mathbf{M}_\\beta &= \\mathbf{\\Omega}^{-1}\\sum_{i=1}^N\\boldsymbol\\delta_i + \\mathbf{\\Lambda}^{-1}\\boldsymbol\\mu\\\\\\\\\n\\omega_{kk}|\\text{``rest\"} &\\sim \\text{InvGamma}(A_k,B_k)\\\\\nA_k &= N/2 + a_k\\\\\nB_k &= \\frac{1}{2}\\sum_{i=1}^N (\\delta_{ik} - \\beta_k)^2 + b_k\\\\\\\\\n\\sigma^2|\\text{\"rest\"} &\\sim \\text{InvGamma}(A,B)\\\\\nA &= \\frac{1}{2}\\sum_{i=1}^N n_i + a\\\\\nB &= \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^{n_i} (Y_{ij} - \\mathbf{X}_i\\boldsymbol\\delta_i)^2 + b\n\\end{align*}\\]\nSimulate some data from this model. In this case will we set \\(N = 100\\), \\(p = 2\\), and \\(n_i = n = 100\\). Then we will implement the Gibbs sampler.\n\n\nSimulate some data\nN <- 100\np <- 2\nn <- rep(100, N)\n\nbeta0  <- rnorm(p, mean = 0, sd = 5)\nOmega0 <- diag(c(2,1))\n\ndelta0  <- t(Rfast::rmvnorm(N, beta0, Omega0))\nsigma20 <- 1\n\nY <- list()\nX <- list()\nfor (i in 1:N){\n  X[[i]] <- matrix(c(rep(1,n[i]), rnorm(n[i]*(p-1))), nrow = n[i], ncol = p)\n  Y[[i]] <- matrix(rnorm(n[i], mean = X[[i]]%*%delta0[,i], sd = sqrt(sigma20)), ncol=1)\n}\n\n\n\n\nRun the Gibbs sampler\n# set-up\nniter <- 5000\nkeep_delta  <- array(NA, dim = c(p, N, niter))\nkeep_beta   <- matrix(NA, nrow = niter, ncol = p)\nkeep_Omega  <- matrix(NA, nrow = niter, ncol = p)\nkeep_sigma2 <- rep(NA, niter)\n\n# initial values\ndelta  <- matrix(0, nrow = p, ncol = N)\nbeta   <- rep(10, p)\nsigma2 <- 3\nOmega  <- diag(c(5,5))\nkeep_delta[,,1] <- delta\nkeep_beta[1,]   <- beta\nkeep_Omega[1,]  <- diag(Omega)\nkeep_sigma2[1]  <- sigma2\n\n\n# prior parameters\nmu    <- rep(0, p)\nLambda_inv <- diag(rep(1e-06,p))\na     <- 0.1\nb     <- 0.1\nAo    <- N/2 + a\nAs    <- sum(n)/2 + a\n\n# pre-computes\nXtX <- list()\nXtY <- list()\nfor (k in 1:N){\n  XtX[[k]] <- t(X[[k]])%*%X[[k]]\n  XtY[[k]] <- t(X[[k]])%*%Y[[k]]\n}\nLmu <- Lambda_inv%*%mu\n\ntic()\n# Gibbs Loop\nfor (i in 2:niter){\n  \n  Omega_inv <- diag(1/diag(Omega))\n  \n  # sample deltas\n  for (k in 1:N){\n    M         <- (1/sigma2)*XtY[[k]] + Omega_inv%*%beta\n    V_inv     <- chol2inv(chol((1/sigma2)*XtX[[k]] + Omega_inv))\n    delta[,k] <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  }\n  \n  # sample beta\n  M     <- Omega_inv%*%rowSums(delta0) + Lmu\n  V_inv <- solve(N*Omega_inv + Lambda_inv)\n  beta  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  \n  # sample omegas\n  for (j in 1:p){\n    Bo <- sum((delta[j,] - beta[j])^2)/2 + b\n    Omega[j,j] <- 1/rgamma(1, Ao, Bo)\n  }\n  \n  # sample sigma2\n  SSE <- 0\n  for (k in 1:N){\n    SSE <- SSE + sum((Y[[k]] - X[[k]]%*%delta[,k])^2)\n  }\n  Bs <- SSE/2 + b\n  sigma2 <- 1/rgamma(1, As, Bs)\n  \n  # store everything\n  keep_delta[,,i] <- delta\n  keep_beta[i,]   <- beta\n  keep_Omega[i,]  <- diag(Omega)\n  keep_sigma2[i]  <- sigma2\n}\ntoc()\n\n\n24.1 sec elapsed\n\n\nThis sampler takes about 20 seconds to run on my machine. That seems slow relative to JAGS. I’m sure there are some computational tricks that I can employ. The way I am computing the overall SSE to update \\(\\sigma^2\\) seems particularly naive.\nInspect some trace plots. The true value of the respective parameter is represented by a red horizontal line. Again, I’ve included the poor initial guess to make the convergence more obvious.\n\n\nConstruct trace plots\nwin <- 1:niter\n\npar(mfrow = c(2,2))\n\nfor (k in 1:p){\n  plot(win, keep_beta[win,k], type = \"l\",\n       ylab = bquote(beta[.(k)]),\n       xlab = \"iter\")\n  abline(h = beta0[k], col = \"red\")\n}\n\nfor (k in 1:p){\n  plot(win, keep_Omega[win, k],   type = \"l\",\n       ylab = bquote(omega[.(k+10*k)]),\n       xlab = \"iter\")\n  abline(h = Omega[k,k], col = \"red\")\n}\n\n\n\n\n\nConstruct trace plots\n# par(mfrow = c(1,1))\n\nplot(win, keep_sigma2[win],   type = \"l\",\n     ylab = bquote(sigma^2),\n     xlab = \"iter\")\nabline(h = sigma20, col = \"red\")"
  },
  {
    "objectID": "posts/2023-02-02-Step-2-Single Parent Design Matrix/index.html",
    "href": "posts/2023-02-02-Step-2-Single Parent Design Matrix/index.html",
    "title": "Step 2 - One Parent Design Matrix",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\nlibrary(Rfast)\n\n\nThe setting is the same as in step 1 only now we will generate a single “parent” design matrix, \\(\\mathbf{X}\\), which is \\(m \\times p\\) and \\(m \\ge n_i\\) for all \\(i\\). That is to say we can’t have more rows in a subject’s design matrix than the parent matrix. To implement this we will generate the parent design matrix, then sample \\(n_i\\) integers from the sequence \\(1,\\dots, m\\) and extract the corresponding rows of \\(\\mathbf{X}\\) to construct a subject’s design matrix, \\(\\widetilde{\\mathbf{X}}_i\\).\nThe parameters \\(\\boldsymbol\\delta_i\\) are drawn from a multivariate normal random effects distribution with mean \\(\\beta\\) and covariance \\(\\mathbf{\\Omega}\\). The entries of \\(\\mathbf{Y}_i\\) are mutually independent with constant variance \\(\\sigma^2\\), \\(\\text{Cov}(\\mathbf{Y}_i) = \\sigma^2 \\mathbf{I}_n\\) for all \\(i\\). Further, \\(\\mathbf{Y}_1,\\dots,\\mathbf{Y}_N\\) are mutually independent.\n\\[\\begin{align*}\n\\mathbf{Y}_i &\\sim \\text{Normal}_{n_i}\\left(\\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i, \\; \\sigma^2 \\mathbf{I}_{n_i}\\right)\\\\\n\\boldsymbol\\delta_i &\\sim \\text{Normal}_p\\left(\\boldsymbol\\beta, \\; \\mathbf{\\Omega}\\right)\n\\end{align*}\\]\nIn this case we will assume \\(\\mathbf{\\Omega}\\) is diagonal and let \\(\\omega_{kk}\\) be the \\(k\\)th diagonal element. Next specify priors, \\[\\begin{align*}\n\\boldsymbol\\beta &\\sim \\text{Normal}_p\\left(\\boldsymbol\\mu, \\; \\mathbf{\\Lambda}\\right)\\\\\n\\omega_{kk} &\\sim \\text{InvGamma}\\left(a_{k}, \\; b_{k} \\right)\\\\\n\\sigma^2 &\\sim \\text{InvGamma}\\left(a, \\; b\\right)\n\\end{align*}\\]\nNote, for simplicity there is no linear trend in the random effects distribution for \\(\\boldsymbol\\delta_i\\). Also, I’ve left its covariance to be diagonal, just to avoid the Inverse Wishart prior for now. This way we can update the diagonal elements of \\(\\mathbf{\\Omega}\\) individually.\nThe full conditionals in this model are. . .\n\\[\\begin{align*}\n\\boldsymbol\\delta_i|\\text{ rest} &\\sim \\text{Normal}_p(\\mathbf{V}_i^{-1}\\mathbf{M}_i, \\mathbf{V}_i^{-1})\\\\\n\\mathbf{V}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\widetilde{\\mathbf{X}}_i + \\mathbf{\\Omega}^{-1}\\\\\n\\mathbf{M}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\mathbf{Y}_i + \\mathbf{\\Omega}^{-1}\\boldsymbol\\beta\\\\\\\\\n\\boldsymbol\\beta|\\text{ rest} &\\sim \\text{Normal}_p(\\mathbf{V}_\\beta^{-1}\\mathbf{M}_\\beta, \\mathbf{V}_\\beta^{-1})\\\\\n\\mathbf{V}_\\beta &= N\\mathbf{\\Omega}^{-1} + \\mathbf{\\Lambda}^{-1}\\\\\n\\mathbf{M}_\\beta &= \\mathbf{\\Omega}^{-1}\\sum_{i=1}^N\\boldsymbol\\delta_i + \\mathbf{\\Lambda}^{-1}\\boldsymbol\\mu\\\\\\\\\n\\omega_{kk}|\\text{ rest} &\\sim \\text{InvGamma}(A_k,B_k)\\\\\nA_k &= N/2 + a_k\\\\\nB_k &= \\frac{1}{2}\\sum_{i=1}^N (\\delta_{ik} - \\beta_k)^2 + b_k\\\\\\\\\n\\sigma^2|\\text{ rest} &\\sim \\text{InvGamma}(A,B)\\\\\nA &= \\frac{1}{2}\\sum_{i=1}^N n_i + a\\\\\nB &= \\frac{1}{2}\\sum_{i=1}^N (\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i)^T(\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i) + b\n\\end{align*}\\]\nSimulate some data from this model. In this case will we set \\(N = 100\\), \\(p = 2\\), \\(n_i = n = 100\\), and \\(m = 400\\).\n\n\nSimulate the data\nm <- 400\nN <- 100\np <- 2\nn <- rep(100, N)\n\nXp <- matrix(c(rep(1,m), rnorm(m*(p-1), mean = 0, sd = 5)), nrow = m, ncol = p)\n\nbeta0  <- rnorm(p, mean = 0, sd = 5)\nOmega0 <- diag(c(2,1))\n\ndelta0  <- t(Rfast::rmvnorm(N, beta0, Omega0))\nsigma20 <- 1\n\nY <- list()\nX <- list()\nfor (i in 1:N){\n  subject_rows <- sample(1:m, n[i])\n  X[[i]] <- Xp[subject_rows,]\n  Y[[i]] <- matrix(rnorm(n[i], mean = X[[i]]%*%delta0[,i], sd = sqrt(sigma20)), ncol=1)\n}\n\n\nThe Gibbs sampler is identical to that found in Step 1.\n\n\nRun the Gibbs Sampler\n# set-up\nniter <- 5000\nkeep_delta  <- array(NA, dim = c(p, N, niter))\nkeep_beta   <- matrix(NA, nrow = niter, ncol = p)\nkeep_Omega  <- matrix(NA, nrow = niter, ncol = p)\nkeep_sigma2 <- rep(NA, niter)\n\n# initial values\ndelta  <- matrix(0, nrow = p, ncol = N)\nbeta   <- rep(10, p)\nsigma2 <- 3\nOmega  <- diag(c(5,5))\nkeep_delta[,,1] <- delta\nkeep_beta[1,]   <- beta\nkeep_Omega[1,]  <- diag(Omega)\nkeep_sigma2[1]  <- sigma2\n\n\n# prior parameters\nmu    <- rep(0, p)\nLambda_inv <- diag(rep(1e-06,p))\na     <- 0.1\nb     <- 0.1\nAo    <- N/2 + a\nAs    <- sum(n)/2 + a\n\n# pre-computes\nXtX <- list()\nXtY <- list()\nfor (k in 1:N){\n  XtX[[k]] <- t(X[[k]])%*%X[[k]]\n  XtY[[k]] <- t(X[[k]])%*%Y[[k]]\n}\nLmu <- Lambda_inv%*%mu\n\ntic()\n# Gibbs Loop\nfor (i in 2:niter){\n  \n  Omega_inv <- diag(1/diag(Omega))\n  \n  # sample deltas\n  for (k in 1:N){\n    M         <- (1/sigma2)*XtY[[k]] + Omega_inv%*%beta\n    V_inv     <- chol2inv(chol((1/sigma2)*XtX[[k]] + Omega_inv))\n    delta[,k] <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  }\n  \n  # sample beta\n  M     <- Omega_inv%*%rowSums(delta0) + Lmu\n  V_inv <- solve(N*Omega_inv + Lambda_inv)\n  beta  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  \n  # sample omegas\n  for (j in 1:p){\n    Bo <- sum((delta[j,] - beta[j])^2)/2 + b\n    Omega[j,j] <- 1/rgamma(1, Ao, Bo)\n  }\n  \n  # sample sigma2\n  SSE <- 0\n  for (k in 1:N){\n    SSE <- SSE + sum((Y[[k]] - X[[k]]%*%delta[,k])^2)\n  }\n  Bs <- SSE/2 + b\n  sigma2 <- 1/rgamma(1, As, Bs)\n  \n  # store everything\n  keep_delta[,,i] <- delta\n  keep_beta[i,]   <- beta\n  keep_Omega[i,]  <- diag(Omega)\n  keep_sigma2[i]  <- sigma2\n}\ntoc()\n\n\n20.45 sec elapsed\n\n\n\n\nConstruct Trace Plots\nwin <- 1:niter\n\npar(mfrow = c(2,2))\n\nfor (k in 1:p){\n  plot(win, keep_beta[win,k], type = \"l\",\n       ylab = bquote(beta[.(k)]),\n       xlab = \"iter\")\n  abline(h = beta0[k], col = \"red\")\n}\n\nfor (k in 1:p){\n  plot(win, keep_Omega[win, k],   type = \"l\",\n       ylab = bquote(omega[.(k)]^2),\n       xlab = \"iter\")\n  abline(h = Omega[k,k], col = \"red\")\n}\n\n\n\n\n\nConstruct Trace Plots\npar(mfrow = c(1,1))\n\nplot(win, keep_sigma2[win],   type = \"l\",\n     ylab = bquote(sigma^2),\n     xlab = \"iter\")\nabline(h = sigma20, col = \"red\")\n\n\n\n\n\n\n\nConstruct Trace Plots\nwin <- 1:niter\n\npar(mfrow = c(2,2))\nparam_sample <- sample(1:100,4)\n\nfor (i in param_sample){\n  subscr <- paste0(\"1,\",i)\n  plot(win, keep_delta[1,i,win], type = \"l\",\n       ylab = bquote(delta[.(subscr)]),\n       xlab = \"iter\")\n  abline(h = delta0[1,i], col = \"red\")\n}\n\n\n\n\n\nConstruct Trace Plots\nparam_sample <- sample(1:100,4)\n\nfor (i in param_sample){\n  subscr <- paste0(\"2,\",i)\n  plot(win, keep_delta[2,i,win], type = \"l\",\n       ylab = bquote(delta[.(subscr)]),\n       xlab = \"iter\")\n  abline(h = delta0[2,i], col = \"red\")\n}\n\n\n\n\n\nOkay, this appears to be working as well based on the trace plots alone. In the next step we will add a linear trend to the underlying distribution for \\(\\boldsymbol\\delta_i\\)."
  },
  {
    "objectID": "posts/2023-02-02-Step-3-Add-a-2nd-Level-Linear-Trend/index.html",
    "href": "posts/2023-02-02-Step-3-Add-a-2nd-Level-Linear-Trend/index.html",
    "title": "Step 3 - Add a Second Level Linear Trend",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\nlibrary(Rfast)\n\n\nBriefly restating notation from earlier, \\(\\mathbf{Y}_i\\) is an \\(n_i \\times 1\\) response vector , \\(\\mathbf{X}\\) an \\(m \\times p\\) parent design matrix, and \\(\\boldsymbol\\delta_i\\) a \\(p \\times 1\\) parameter vector corresponding to subject \\(i = 1,\\dots,N\\). The matrix \\(\\widetilde{\\mathbf{X}}_i\\) is an \\(n_i \\times p\\) matrix associated with subject \\(i\\) which is construct with a sample of rows from the parent design matrix \\(\\mathbf{X}\\). The entries of \\(\\mathbf{Y}_i\\) are mutually independent with constant variance \\(\\sigma^2\\), \\(\\text{Cov}(\\mathbf{Y}_i) = \\sigma^2 \\mathbf{I}_{n_i}\\) for all \\(i\\). Further, \\(\\mathbf{Y}_1,\\dots,\\mathbf{Y}_N\\) are mutually independent.\nIn this step we will add a linear trend to the mean of the distribution for \\(\\boldsymbol\\delta_i\\). There will be more parameters to estimate and some modification to the full conditionals for the Gibbs sampler. Let \\(\\mathbf{z}_i\\) be a \\(q \\times 1\\) vector of covariates, including an intercept element in the first position.\nThe linear trend in the random effects distribution for \\(\\boldsymbol\\delta_i\\) is represented as a linear combination \\(\\sum_{l=1}^q\\boldsymbol\\beta_l z_{il}\\) where \\(\\boldsymbol\\beta_l\\) are \\(p \\times 1\\) vectors. There are two ways to write this expression in matrix notation. Let \\(\\boldsymbol\\beta = (\\boldsymbol\\beta_1^T, \\boldsymbol\\beta_2^T, \\dots, \\boldsymbol\\beta_q^T)^T\\) be the \\(qp \\times 1\\) vector of stacked \\(\\boldsymbol\\beta_i\\) vectors and let \\(\\mathbf{B} = [\\boldsymbol\\beta_1 \\;\\;\\; \\boldsymbol\\beta_2 \\;\\;\\; \\dots \\;\\;\\; \\boldsymbol\\beta_q]\\) be the \\(p \\times q\\) matrix of arranged \\(\\boldsymbol\\beta_i\\) vectors. Note, \\(\\boldsymbol\\beta = \\text{vec}(\\mathbf{B})\\). Either we write\n\\[\\begin{equation}\n\\tag{1}\n\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta =\n\\begin{bmatrix}\n1 & \\dots & 0 & z_{i2} & \\dots & 0 & & z_{iq} & \\dots & 0\\\\\n\\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots & \\dots & \\vdots & \\ddots & \\vdots\\\\\n0 & \\dots & 1 & 0 & \\dots & z_{i2} & & 0 & \\dots & z_{iq}\n\\end{bmatrix}_{p \\, \\times \\, qp}\n\\begin{bmatrix}\n\\boldsymbol\\beta_1\\\\\n\\boldsymbol\\beta_2\\\\\n\\vdots\\\\\n\\boldsymbol\\beta_q\n\\end{bmatrix}_{qp \\, \\times \\, 1}\n\\end{equation}\\] or we could write \\[\\begin{equation}\n\\tag{2}\n\\mathbf{B}\\mathbf{z}_i =\n\\begin{bmatrix}\n\\boldsymbol\\beta_1 & \\boldsymbol{\\beta}_2 & \\dots & \\boldsymbol\\beta_q\n\\end{bmatrix}_{p \\,\\times \\, q}\n\\begin{bmatrix}\n1\\\\\nz_{i2}\\\\\n\\vdots\\\\\nz_{iq}\n\\end{bmatrix}_{q \\, \\times \\, 1}\n\\end{equation}\\]\nThe former is attractive for deriving analytical expressions and the latter is attractive for some computational advantages. The model is defined to be as follows,\n\\[\\begin{align*}\n\\mathbf{Y}_i &\\sim \\text{Normal}_{n_i}\\left(\\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i, \\; \\sigma^2 \\mathbf{I}_{n_i}\\right)\\\\\n\\boldsymbol\\delta_i &\\sim \\text{Normal}_p\\left(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta, \\; \\mathbf{\\Omega}\\right)\n\\end{align*}\\]\nAgain, we will assume \\(\\mathbf{\\Omega}\\) is diagonal and let \\(\\omega_{kk}\\) be the \\(k\\)th diagonal element. Next we specify priors, \\[\\begin{align*}\n\\boldsymbol\\beta &\\sim \\text{Normal}_{qp}\\left(\\boldsymbol\\mu, \\; \\mathbf{\\Lambda}\\right)\\\\\n\\omega_{kk} &\\sim \\text{InvGamma}\\left(a_\\omega, \\; b_\\omega \\right)\\\\\n\\sigma^2 &\\sim \\text{InvGamma}\\left(a_\\sigma, \\; b_\\sigma\\right)\n\\end{align*}\\]\nThe full conditionals in this model are as follows,\n\\[\\begin{align*}\n\\boldsymbol\\delta_i|\\text{ rest} &\\sim \\text{Normal}_p(\\mathbf{V}_i^{-1}\\mathbf{M}_i, \\mathbf{V}_i^{-1})\\\\\n\\mathbf{V}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\widetilde{\\mathbf{X}}_i + \\mathbf{\\Omega}^{-1}\\\\\n\\mathbf{M}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\mathbf{Y}_i + \\mathbf{\\Omega}^{-1}\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta\\\\\\\\\n\\boldsymbol\\beta|\\text{ rest} &\\sim \\text{Normal}_p(\\mathbf{V}_\\beta^{-1}\\mathbf{M}_\\beta, \\mathbf{V}_\\beta^{-1})\\\\\n\\mathbf{V}_\\beta &= \\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\mathbf{\\Omega}^{-1}\\boldsymbol{\\mathcal{Z}}_i + \\mathbf{\\Lambda}^{-1}\\\\\n\\mathbf{M}_\\beta &= \\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\mathbf{\\Omega}^{-1}\\boldsymbol\\delta_i + \\mathbf{\\Lambda}^{-1}\\boldsymbol\\mu\\\\\\\\\n\\omega_{kk}|\\text{ rest} &\\sim \\text{InvGamma}(A_\\omega,B_\\omega)\\\\\nA_\\omega &= N/2 + a_\\omega\\\\\nB_\\omega &= \\frac{1}{2}\\sum_{i=1}^N (\\delta_{ik} - (\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta)_k)^2 + b_\\omega\\\\\\\\\n\\sigma^2|\\text{ rest} &\\sim \\text{InvGamma}(A_\\sigma,B_\\sigma)\\\\\nA_\\sigma &= \\frac{1}{2}\\sum_{i=1}^N n_i + a_\\sigma\\\\\nB_\\sigma &= \\frac{1}{2}\\sum_{i=1}^N (\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i)^T(\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i) + b_\\sigma\n\\end{align*}\\]\nA few computational remarks, in the full conditional for \\(\\boldsymbol\\beta\\), we have the expression \\(\\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i\\) which would be much too naive to compute directly for each MCMC iteration. Instead, we will rewrite this expression in such a way that will allow us to simplify computations inside the Gibbs loop. First, consider a different and perhaps more natural organization of the covariates \\(\\mathbf{z}_i\\), into a matrix \\(\\mathbf{Z} = (\\mathbf{z}_1^T, \\dots,\\mathbf{z}_N^T)^T\\) \\[\\begin{equation}\n\\tag{3}\n\\mathbf{Z} =\n\\begin{bmatrix}\n1 & z_{12} & \\dots & z_{1q}\\\\\n\\vdots & & \\vdots &\\\\\n1 & z_{N2} & \\dots & z_{Nq}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{z}_1^T\\\\\n\\vdots\\\\\n\\mathbf{z}_N^T\\\\\n\\end{bmatrix}\n\\end{equation}\\]\nLet \\(\\otimes\\) represent the Kronecker product. The following identity holds, \\[\\begin{equation}\n\\tag{4}\n\\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\boldsymbol\\Omega^{-1}\\boldsymbol{\\mathcal{Z}}_i = \\mathbf{Z}^T\\mathbf{Z} \\otimes \\boldsymbol\\Omega^{-1}\n\\end{equation}\\]\nWith \\(\\mathbf{Z}\\) known we can compute \\(\\mathbf{Z}^T\\mathbf{Z}\\) outside of the Gibbs loop.\nAlso in the full conditional for \\(\\boldsymbol\\beta\\) we have the expression $ _{i=1}NT_i^{-1}_i$. Unfortunately, with \\(\\boldsymbol\\delta_i\\) and \\(\\boldsymbol\\Omega^{-1}\\) being parameters to update, this expression must be fully evaluated at each MCMC iteration. Let \\(\\boldsymbol\\Delta = (\\boldsymbol\\delta_1,\\dots,\\boldsymbol\\delta_N)\\) be the \\(p \\times N\\) matrix of arranged parameter vectors, \\(\\boldsymbol\\delta_i\\), and \\(\\mathbf{Z}\\) be defined as before in \\((3)\\). A useful identity in computing the quantity of interest is \\[\\begin{equation}\n\\tag{5}\n\\sum_{i=1}^N\\boldsymbol{\\mathcal{Z}}^T_i\\mathbf{\\Omega}^{-1}\\boldsymbol\\delta_i =  \\text{vec}(\\Omega^{-1}\\boldsymbol\\Delta\\mathbf{Z}) = (\\mathbf{Z}^T \\otimes \\, \\boldsymbol\\Omega^{-1})\\text{vec}(\\boldsymbol\\Delta).\n\\end{equation}\\]\nFor the other full conditionals in which \\(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta\\) appears, namely \\(\\boldsymbol\\delta_i\\) and \\(\\omega_{kk}\\), we can replace with appropriate and readily computed form \\(\\mathbf{B}\\mathbf{z}_i\\) as defined in \\((2)\\).\nSimulate some data from this model. In this case will we set \\(p = 2\\), \\(q = 2\\), \\(N = 100\\), \\(n_i = n = 100\\), and \\(m = 400\\).\n\n\nSimulate the data\n# dimensions\nN <- 100\nm <- 400\nn <- rep(100, N)\np <- 2\nq <- 2\n\n# Design matrices\nXp <- matrix(c(rep(1,m), rnorm(m*(p-1), mean = 0, sd = 5)), nrow = m, ncol = p)\nZ  <- matrix(c(rep(1,N), rnorm(N*(q-1), mean = 0, sd = 5)), nrow = N, ncol = q)\n\n# beta parameters\nB0     <- Rfast::rmvnorm(q, rep(0,p), (5^2)*diag(2))\nbeta0  <- matrix(c(B0), ncol = 1)\nOmega0 <- diag(c(2,1))\n\n# delta parameters\ndelta0  <- matrix(0, nrow = p, ncol = N)\nsigma20 <- 1\n\n# sample data\nY <- list()\nX <- list()\nfor (i in 1:N){\n  # draw delta\n  delta0[,i]   <- t(Rfast::rmvnorm(1, B0%*%Z[i,], Omega0))\n  \n  # draw rows from parent X\n  subject_rows <- sample(1:m, n[i])\n  X[[i]] <- Xp[subject_rows,]\n  \n  # draw response\n  Y[[i]] <- matrix(rnorm(n[i], mean = X[[i]]%*%delta0[,i], sd = sqrt(sigma20)), ncol=1)\n}\n\n\n\n\nRun the Gibbs Sampler\n# set-up\nniter <- 5000\nkeep_delta  <- array(NA, dim = c(p, N, niter))\nkeep_B      <- array(NA, dim = c(p, q, niter))\nkeep_Omega  <- matrix(NA, nrow = niter, ncol = p)\nkeep_sigma2 <- rep(NA, niter)\n\n# initial values\ndelta  <- matrix(0, nrow = p, ncol = N)\nbeta   <- rep(10, q*p)\nB      <- matrix(c(beta), nrow=p)\nsigma2 <- 3\nOmega  <- diag(c(5,5))\nkeep_delta[,,1] <- delta\nkeep_B[,,1]     <- B\nkeep_Omega[1,]  <- diag(Omega)\nkeep_sigma2[1]  <- sigma2\n\n# prior parameters\nmu    <- rep(0, q*p)\nLambda_inv <- diag(rep(1e-06,q*p))\na     <- 0.1\nb     <- 0.1\n\n# pre-computes\nXtX <- list()\nXtY <- list()\nfor (k in 1:N){\n  XtX[[k]] <- t(X[[k]])%*%X[[k]]\n  XtY[[k]] <- t(X[[k]])%*%Y[[k]]\n}\nZtZ <- t(Z)%*%Z\nLmu <- Lambda_inv%*%mu\nAo    <- N/2 + a\nAs    <- sum(n)/2 + a\n\ntic()\n# Gibbs Loop\nfor (iter in 2:niter){\n\n  Omega_inv <- diag(1/diag(Omega))\n  \n  # sample deltas\n  for (i in 1:N){\n    M         <- (1/sigma2)*XtY[[i]] + Omega_inv%*%B%*%Z[i,]\n    V_inv     <- chol2inv(chol((1/sigma2)*XtX[[i]] + Omega_inv))\n    delta[,i] <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  }\n  \n  # sample beta\n  M     <- kronecker(t(Z), Omega_inv)%*%matrix(c(delta), ncol = 1) + Lmu\n  V_inv <- solve(kronecker(ZtZ, Omega_inv) + Lambda_inv)\n  beta  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(q*p)\n  B     <- matrix(beta, nrow = p)\n  \n  # sample omegas\n  for (k in 1:p){\n    Bo <- sum((delta[k,] - (B%*%t(Z))[k,])^2)/2 + b\n    Omega[k,k] <- 1/rgamma(1, Ao, Bo)\n  }\n  \n  # sample sigma2\n  SSE <- 0\n  for (i in 1:N){\n    SSE <- SSE + sum((Y[[i]] - X[[i]]%*%delta[,i])^2)\n  }\n  Bs <- SSE/2 + b\n  sigma2 <- 1/rgamma(1, As, Bs)\n  \n  # store everything\n  keep_delta[,,iter] <- delta\n  keep_B[,,iter]     <- B\n  keep_Omega[iter,]  <- diag(Omega)\n  keep_sigma2[iter]  <- sigma2\n}\ntoc()\n\n\n21.13 sec elapsed\n\n\nNow for some trace plots. The following are for \\(\\mathbf{B}\\), \\(\\boldsymbol\\Omega\\), and \\(\\sigma^2\\). And display iterations 100:5000. This visual inspection seems to indicate good convergence!\n\n\nConstruct Trace Plots\nwin <- 100:niter\n\npar(mfrow = c(2,2))\n\nfor(l in 1:q){\n  for (k in 1:p){\n    plot(win, keep_B[k, l, win], type = \"l\",\n         ylab = bquote(beta[paste(.(l),\",\",.(k))]),\n         xlab = \"iter\")\n    abline(h = B0[k, l], col = \"red\")\n  }\n}\n\n\n\n\n\nConstruct Trace Plots\nfor (k in 1:p){\n  plot(win, keep_Omega[win, k], type = \"l\",\n       ylab = bquote(omega[paste(.(k),\",\",.(k))]),\n       xlab = \"iter\")\n  abline(h = Omega[k, k], col = \"red\")\n}\n\nplot(win, keep_sigma2[win], type = \"l\",\n     ylab = bquote(sigma^2),\n     xlab = \"iter\")\nabline(h = sigma20, col = \"red\")"
  },
  {
    "objectID": "posts/2023-02-02-Step-4-Application-to-the-Linearized-Double-Logistic-Function/index.html",
    "href": "posts/2023-02-02-Step-4-Application-to-the-Linearized-Double-Logistic-Function/index.html",
    "title": "Step 4 - Application to the Linearized Double-Logistic Function",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\nlibrary(Rfast)\nlibrary(numDeriv)\n\n\nLet \\(Y_{ij}\\) be an observation collected on day \\(t_{ij}\\) of year \\(i\\), \\(i = 1, \\dots, N\\) and \\(j = 1,\\dots,n_i\\). Assume \\(Y_{ij}\\) to be normally distributed with mean \\(v(t_{ij}, \\, \\boldsymbol\\theta_i)\\) and constant variance \\(\\sigma^2\\). The mean “double-logistic” function, \\(v(t, \\, \\boldsymbol{\\theta})\\), is parameterized by \\(\\boldsymbol\\theta\\) and is defined \\[\\begin{equation}\n\\tag{1}\nv(t, \\, \\boldsymbol\\theta) = \\theta_1 + (\\theta_2 - \\theta_7 t)\\left(\\frac{1}{\\exp\\left\\{\\frac{\\theta_3 - t}{\\theta_4}\\right\\}} - \\frac{1}{\\exp\\left\\{\\frac{\\theta_4 - t}{\\theta_6}\\right\\}}\\right)\n\\end{equation}\\]\nThere is an additional constraint that \\(v(t, \\, \\boldsymbol{\\theta}) \\in [0,1]\\) and \\(\\theta_7 \\ge 0\\). One option would be to put priors on \\(\\theta_1\\) with \\([0,1]\\) support and \\(\\theta_7\\) with \\([0,\\infty)\\) support. One might also consider the constraint \\(\\theta_2 \\in [\\theta_1, 1]\\), but we should be careful to specify a prior whose support depends on another parameter. Instead, we opt not to constrain or transform \\(\\theta_2\\). In general, we would prefer to place a multivariate normal prior over the entire vector \\(\\boldsymbol\\theta\\). To that end, consider a reparameterazation of \\(v(t,\\boldsymbol\\theta)\\),\n\\[\\begin{equation}\n\\tag{2}\nv(t, \\, \\boldsymbol\\theta) = \\frac{1}{1+\\exp\\{\\theta_1\\}} + \\left(\\theta_2 - \\exp\\{\\theta_7\\} t\\right)\\left(\\frac{1}{\\exp\\left\\{\\frac{\\theta_3 - t}{\\theta_4}\\right\\}} - \\frac{1}{\\exp\\left\\{\\frac{\\theta_4 - t}{\\theta_6}\\right\\}}\\right)\n\\end{equation}\\]\nThe function \\(v(t, \\, \\boldsymbol{\\theta})\\) is a non-linear function of \\(\\boldsymbol\\theta\\) which consequently violates conjugacy of the typically assumed priors. To overcome this we will work with a linearized version of \\(v(t, \\, \\boldsymbol{\\theta})\\). This significantly decreases the computational burden of MCMC at the cost of introducing statistical bias. Are the computational gains worth the induced bias?\nThe linearization of \\(v(t, \\, \\boldsymbol\\theta)\\) around \\(\\boldsymbol\\theta_0\\) yields, \\[\\begin{equation}\n\\tag{3}\nv(t, \\, \\boldsymbol\\theta) \\approx v(t, \\, \\boldsymbol\\theta_0) + \\nabla_{\\boldsymbol\\theta}v(t, \\, \\boldsymbol\\theta)|_{\\boldsymbol\\theta=\\boldsymbol\\theta_0}(\\boldsymbol\\theta - \\boldsymbol\\theta_0)\n\\end{equation}\\]\nDefine the following,\n\\[\\begin{equation}\n\\tag{4}\nY^*_{ij} = Y_{ij} - v(t_{ij}, \\, \\boldsymbol\\theta_0)\n\\end{equation}\\]\n\\[\\begin{equation}\n\\tag{5}\nX(t) = \\nabla_{\\boldsymbol\\theta}v(t, \\, \\boldsymbol\\theta)|_{\\boldsymbol\\theta=\\boldsymbol\\theta_0}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\tag{6}\n\\boldsymbol\\delta_i = \\boldsymbol\\theta_i-\\boldsymbol\\theta_0\n\\end{equation}\\]\n\\[\\begin{equation}\n\\mathbf{Y}_i \\sim \\text{Normal}_{n_i}\\left(v(\\mathbf{t}_i, \\boldsymbol{\\theta}_i), \\; \\sigma^2 \\mathbf{I}_{n_i}\\right)\n\\end{equation}\\]\nConstruct the \\(m \\times p\\) parent design matrix \\(\\mathbf{X}\\) with rows \\(X(t)\\) for \\(t=1,\\dots,m\\). Define the “child” design matrix, \\(\\widetilde{\\mathbf{X}}_i\\) to be the subset of rows from \\(\\mathbf{X}\\) specified by \\(\\mathbf{t}_i\\).\nThe \\(N \\times q\\) covariate matrix \\(\\mathbf{Z}\\) organizes known covariates for the mean of the distribution for \\(\\boldsymbol\\delta_i\\). Let \\(\\mathbf{z}_i\\) be the \\(i\\)th row of \\(\\mathbf{Z}\\) and \\(\\boldsymbol{\\mathcal{Z}}_i = (\\mathbf{z}^T_i\\otimes \\mathbf{I}_p)\\) where “\\(\\otimes\\)” represents the Kronecker product.\nNow we can place a multivariate prior on \\(\\boldsymbol\\delta_i\\) as this parameter represents deviations from \\(\\boldsymbol\\theta_0\\). The linearized model is as follows,\n\\[\\begin{align*}\n\\mathbf{Y}^*_i &\\sim \\text{Normal}_{n_i}\\left(\\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i, \\; \\sigma^2 \\mathbf{I}_{n_i}\\right)\\\\\n\\boldsymbol\\delta_i &\\sim \\text{Normal}_p\\left(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta, \\; \\mathbf{\\Omega}\\right)\n\\end{align*}\\]\nAgain, we will assume \\(\\mathbf{\\Omega}\\) is diagonal and let \\(\\omega_{kk}\\) be the \\(k\\)th diagonal element. Next we specify priors, \\[\\begin{align*}\n\\boldsymbol\\beta &\\sim \\text{Normal}_{qp}\\left(\\boldsymbol\\mu, \\; \\mathbf{\\Lambda}\\right)\\\\\n\\omega_{kk} &\\sim \\text{InvGamma}\\left(a_\\omega, \\; b_\\omega \\right)\\\\\n\\sigma^2 &\\sim \\text{InvGamma}\\left(a_\\sigma, \\; b_\\sigma\\right)\n\\end{align*}\\]\nLet \\(\\boldsymbol\\Delta = [\\boldsymbol\\delta_1 \\; \\dots \\; \\boldsymbol\\delta_N]\\) and \\(\\mathbf{B} = [\\boldsymbol\\beta_1 \\; \\dots \\; \\boldsymbol\\beta_q]\\). That is, \\(\\mathbf{B}\\) is a \\(p \\times q\\) matrix and \\(\\text{vec}(\\mathbf{B}) = \\boldsymbol\\beta\\). The full conditionals in this model are as follows,\n\\[\\begin{align*}\n\\boldsymbol\\delta_i|\\text{rest} &\\sim \\text{Normal}_p(\\mathbf{V}_i^{-1}\\mathbf{M}_i, \\mathbf{V}_i^{-1})\\\\\n\\mathbf{V}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\widetilde{\\mathbf{X}}_i + \\mathbf{\\Omega}^{-1}\\\\\n\\mathbf{M}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\mathbf{Y}_i + \\mathbf{\\Omega}^{-1}\\mathbf{B}\\mathbf{z}_i\\\\\\\\\n\\boldsymbol\\beta|\\text{rest} &\\sim \\text{Normal}_{qp}(\\mathbf{V}_\\beta^{-1}\\mathbf{M}_\\beta, \\mathbf{V}_\\beta^{-1}\\\\\n\\mathbf{V}_\\beta &= \\mathbf{Z}^T\\mathbf{Z} \\otimes \\boldsymbol\\Omega^{-1} + \\mathbf{\\Lambda}^{-1}\\\\\n\\mathbf{M}_\\beta &= (\\mathbf{Z}^T \\otimes \\, \\boldsymbol\\Omega^{-1})\\text{vec}(\\boldsymbol\\Delta) + \\mathbf{\\Lambda}^{-1}\\boldsymbol\\mu\\\\\\\\\n\\omega_{kk}|\\text{rest} &\\sim \\text{InvGamma}(A_\\omega,B_\\omega)\\\\\nA_\\omega &= N/2 + a_\\omega\\\\\nB_\\omega &= \\frac{1}{2}\\sum_{i=1}^N (\\delta_{ik} - (\\mathbf{B}\\mathbf{z}_i)_k)^2 + b_\\omega\\\\\\\\\n\\sigma^2|\\text{rest} &\\sim \\text{InvGamma}(A_\\sigma,B_\\sigma)\\\\\nA_\\sigma &= \\frac{1}{2}\\sum_{i=1}^N n_i + a_\\sigma\\\\\nB_\\sigma &= \\frac{1}{2}\\sum_{i=1}^N (\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i)^T(\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i) + b_\\sigma\n\\end{align*}\\]\nNext moving on the code implementation. Define the double logistic function and its gradient.\n\n\nUser-defined functions.\n# expit #\n# equivalent to plogis with m=0 & s=1.\nexpit   <- function(x){1/(1+exp(-x))}\n\n# expit_p #\n# first derivative of the expit function. equivalent to dlogis with m=0 & s=1.\nexpit_p <- function(x){expit(x)*(1-expit(x))}\n\n\ndouble_logis <- function(t, theta){\n  # double logistic function.\n  # theta1 is transformed using the logistic function.\n  # theta2 is transformed using the \n  # This allows for all parameters to follow a gaussian distribution\n  \n  theta[1] <- plogis(theta[1])\n  theta[7] <- exp(theta[7])  \n  \n  n1 <- 1\n  d1 <- 1 + exp((theta[3] - t)/theta[4])\n    \n  n2 <- 1\n  d2 <- 1 + exp((theta[5] - t)/theta[6])\n    \n  out <- theta[1] + (theta[2] - theta[7]*t)*(n1/d1 - n2/d2)\n  \n  return(out)\n}\n\n\n# double logistic gradient wrt theta\nbasis_functions <- function(t,theta){\n  t   <- t%%366\n\n  dl0 <- double_logis(t,theta)-expit(theta[1])\n  a <- expit_p(theta[7])\n  theta[7] <- exp(theta[7])\n\n  B1 <- expit_p(theta[1])\n  B2 <- (1/(1+exp((theta[3]-t)/theta[4])))-(1/(1+exp((theta[5]-t)/theta[6])))\n  B3 <- (theta[7]*t - theta[2])/(2*theta[4]*cosh((theta[3]-t)/theta[4])+2*theta[4])\n  B4 <- ((theta[3] - t)*(theta[2]-theta[7]*t)*cosh((theta[3]-t)/(2*theta[4]))^(-2))/(4*theta[4]^2)\n  B5 <- (theta[2] - theta[7]*t)/(2*theta[6]*cosh((theta[5]-t)/theta[6])+2*theta[6])\n  B6 <- ((theta[5] - t)*(theta[7]*t-theta[2])*cosh((theta[5]-t)/(2*theta[6]))^(-2))/(4*theta[6]^2)\n  B7 <- -t*a*((1/(1+exp((theta[3]-t)/theta[4])))-(1/(1+exp((theta[5]-t)/theta[6]))))\n  B <- unname(cbind(B1, B2, B3, B4, B5, B6, B7))\n\n  return(B)\n}\n\n\nWe need to specify a value of \\(\\boldsymbol\\theta_0\\) on which to center the linearization. We will expand on how to obtain this value at a later time.\n\n\nCode\ntheta0 <- c(-1.80, 0.75, 120, 8, 270, 8, -7.5)\n\nplot(1:366, double_logis(1:366, theta0), type = \"l\",\n     xlab = \"t\",\n     ylab = bquote(v(t,theta[0])))\n\n\n\n\n\nWe verify that the analytical gradient is correct by checking the result against a numerical gradient function.\n\n\nConstruct the numerical and analytical gradiant matrix\ntheta0 <- c(-1.80, 0.75, 120, 8, 270, 8, -7.5)\n\ngradinput <- function(x,t){\n  return(double_logis(t,x))\n}\n\n# compute numerical gradient\nG <- matrix(0, nrow = 366, ncol = 7)\nfor (i in 1:366){\n  G[i,] <- grad(gradinput, x = theta0, t=i)\n}\n\n# # compute the analytical gradient\nX <- basis_functions(1:366, theta0)\n\nmatplot(G, type=\"l\", main = \"Numerical Gradient\", xlab = \"t\")\n\n\n\n\n\nConstruct the numerical and analytical gradiant matrix\nmatplot(X, type=\"l\", main = \"Analytical Gradient\", xlab = \"t\")\n\n\n\n\n\nNext we simulate some data from the linearized model. (Need to add details regarding simulation set-up).\n\n\nSimulate the data\n# dimensions\nN <- 40\nm <- 366\nn <- rep(100,N)\np <- 7\nq <- 2\n\ntheta0 <- c(-1.80, 0.75, 120, 8, 270, 8, -7.5)\n\n# Design matrices\ngradinput <- function(x,t){\n  return(double_logis(t,x))\n}\n\n# compute numerical gradient\nXp <- basis_functions(1:366, theta0)\nZ  <- matrix(c(rep(1,N), seq(1:N)), ncol = 2)\n\n# beta parameters\nB0     <- matrix(c(0, 0, -15, 0, 15, 0, 0,\n                   0, 0,   1, 0, -1, 0, 0), nrow=p)\nbeta0  <- matrix(c(B0), ncol = 1)\nOmega0 <- diag(c(0.025, 1e-3,  3, 1,  3, 1, 0.0005))\n\n# delta parameters\ndelta0  <- matrix(0, nrow = p, ncol = N)\nsigma20 <- 0.0025\n\n# sample data\nY <- list()\nX <- list()\nt <- list()\nfor (i in 1:N){\n  # draw delta\n  delta0[,i]   <- t(Rfast::rmvnorm(1, B0%*%Z[i,], Omega0))\n  \n  # draw rows from parent X\n  t[[i]] <- sample(1:m, n[i])\n  X[[i]] <- Xp[t[[i]],]\n  \n  # draw response\n  Y[[i]] <- matrix(rnorm(n[i], mean = X[[i]]%*%delta0[,i], sd = sqrt(sigma20)), ncol=1)\n}\n\n\nPlot the simulated curves. We won’t plot the individual observations.\n\n\nCode\nplot(1:366, double_logis(1:366, theta0), type = \"l\", ylim=c(-0.1, 1.2))\n\nfor (i in 1:N){\n  lines(1:366, double_logis(1:366, theta0) + Xp%*%delta0[,i], type=\"l\", col = i,\n        xlab = \"t\",\n        ylab = \"Y\",\n        main = \"Simulated Curves\")\n  # points(t[[i]], Y[[i]] + double_logis(t[[i]], theta0), pch=16)\n}\n\n\n\n\n\nThe Gibbs loop. Later we will re-write this as a function.\n\n\nRun the Gibbs sampler\n# set-up\nniter <- 5000\nkeep_delta  <- array(NA, dim = c(p, N, niter))\nkeep_B      <- array(NA, dim = c(p, q, niter))\nkeep_Omega  <- matrix(NA, nrow = niter, ncol = p)\nkeep_sigma2 <- rep(NA, niter)\n\n# initial values\ndelta  <- matrix(0, nrow = p, ncol = N)\nbeta   <- rep(0, q*p)\nB      <- matrix(c(beta), nrow=p)\nsigma2 <- 0.01\nOmega  <- diag(c(0.5, 0.5, 5, 0.5, 5, 0.5, 0.001))\nkeep_delta[,,1] <- delta\nkeep_B[,,1]     <- B\nkeep_Omega[1,]  <- diag(Omega)\nkeep_sigma2[1]  <- sigma2\n\n# prior parameters\nmu    <- rep(0, q*p)\nLambda_inv <- diag(rep(1e-06,q*p))\na     <- 0.1\nb     <- 0.1\n\n# pre-computes\nXtX <- list()\nXtY <- list()\nfor (k in 1:N){\n  XtX[[k]] <- t(X[[k]])%*%X[[k]]\n  XtY[[k]] <- t(X[[k]])%*%Y[[k]]\n}\nZtZ <- t(Z)%*%Z\nLmu <- Lambda_inv%*%mu\nAo    <- N/2 + a\nAs    <- sum(n)/2 + a\n\n# Gibbs Loop\ntic()\nfor (iter in 2:niter){\n\n  Omega_inv <- diag(1/diag(Omega))\n  \n  # sample deltas\n  for (i in 1:N){\n    M         <- (1/sigma2)*XtY[[i]] + Omega_inv%*%(B%*%Z[i,])\n    V_inv     <- chol2inv(chol((1/sigma2)*XtX[[i]] + Omega_inv))\n    delta[,i] <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  }\n  \n  # sample beta\n  M     <- kronecker(t(Z), Omega_inv)%*%matrix(c(delta), ncol = 1) + Lmu\n  V_inv <- solve(kronecker(ZtZ, Omega_inv) + Lambda_inv)\n  beta  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(q*p)\n  B     <- matrix(beta, nrow = p)\n  \n  # sample omegas\n  for (k in 1:p){\n    Bo <- sum((delta[k,] - (B%*%t(Z))[k,])^2)/2 + b\n    Omega[k,k] <- 1/rgamma(1, Ao, Bo)\n  }\n  \n  # sample sigma2\n  SSE <- 0\n  for (i in 1:N){\n    SSE <- SSE + sum((Y[[i]] - X[[i]]%*%delta[,i])^2)\n  }\n  Bs <- SSE/2 + b\n  sigma2 <- 1/rgamma(1, As, Bs)\n  \n  # store everything\n  keep_delta[,,iter] <- delta\n  keep_B[,,iter]     <- B\n  keep_Omega[iter,]  <- diag(Omega)\n  keep_sigma2[iter]  <- sigma2\n}\ntoc()\n\n\n11.15 sec elapsed\n\n\nNow for some trace plots. The following are for \\(\\mathbf{B}\\), \\(\\boldsymbol\\Omega\\), and \\(\\sigma^2\\). And display iterations 100:5000. This visual inspection seems to indicate good convergence!\n\n\nConstruct trace plots\nwin <- 1:niter\n\npar(mfrow = c(2,2))\n\nfor(l in 1:q){\n  for (k in 1:p){\n    plot(win, keep_B[k, l, win], type = \"l\",\n         ylab = bquote(beta[paste(.(l),\",\",.(k))]),\n         xlab = \"iter\")\n    abline(h = B0[k, l], col = \"red\")\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\nConstruct trace plots\nfor (k in 1:p){\n  plot(win, keep_Omega[win, k], type = \"l\",\n       ylab = bquote(omega[paste(.(k),\",\",.(k))]),\n       xlab = \"iter\")\n  abline(h = Omega0[k, k], col = \"red\")\n}\n\n\n\n\n\n\n\n\nConstruct trace plots\nplot(win, keep_sigma2[win], type = \"l\",\n     ylab = bquote(sigma^2),\n     xlab = \"iter\")\nabline(h = sigma20, col = \"red\")\n\n\n\n\n\nTrace plots for 5 subject \\(\\boldsymbol\\delta_i\\) chosen at random.\n\n\nConstruct trace plots\nwin <- 1:niter\n\npar(mfrow = c(2,2))\nparam_sample <- sample(1:N, 5)\n\nfor (year in param_sample){\n  for (k in 1:p){\n    subscr <- paste0(year,\",\",k)\n    plot(win, keep_delta[k, year, win], type = \"l\",\n         ylab = bquote(delta[.(subscr)]),\n         xlab = \"iter\")\n    abline(h = delta0[k, year], col = \"red\")\n  }\n}"
  },
  {
    "objectID": "posts/2023-02-02-Step-5-Verify-Convergence/index.html",
    "href": "posts/2023-02-02-Step-5-Verify-Convergence/index.html",
    "title": "Step 5 - Verify Convergence",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\nlibrary(Rfast)\nlibrary(numDeriv)\n\n\nThe goal here will be to verify convergence of the MCMC developed in the previous steps. First, we’ll define the functions necessary to fit the model.\n\n\nUser-defined functions\n# expit #\n# equivalent to plogis with m=0 & s=1.\nexpit   <- function(x){1/(1+exp(-x))}\n\n# expit_p #\n# first derivative of the expit function. equivalent to dlogis with m=0 & s=1.\nexpit_p <- function(x){expit(x)*(1-expit(x))}\n\n\ndouble_logis <- function(t, theta){\n  # double logistic function.\n  # theta1 is transformed using the logistic function.\n  # theta2 is transformed using the \n  # This allows for all parameters to follow a gaussian distribution\n  \n  theta[1] <- plogis(theta[1])\n  theta[7] <- exp(theta[7])  \n  \n  n1 <- 1\n  d1 <- 1 + exp((theta[3] - t)/theta[4])\n  \n  n2 <- 1\n  d2 <- 1 + exp((theta[5] - t)/theta[6])\n  \n  out <- theta[1] + (theta[2] - theta[7]*t)*(n1/d1 - n2/d2)\n  \n  return(out)\n}\n\n# double logistic gradient wrt theta\nbasis_functions <- function(t,theta){\n  t   <- t%%366\n  \n  dl0 <- double_logis(t,theta)-expit(theta[1])\n  a <- expit_p(theta[7])\n  theta[7] <- exp(theta[7])\n  \n  B1 <- expit_p(theta[1])\n  B2 <- (1/(1+exp((theta[3]-t)/theta[4])))-(1/(1+exp((theta[5]-t)/theta[6])))\n  B3 <- (theta[7]*t - theta[2])/(2*theta[4]*cosh((theta[3]-t)/theta[4])+2*theta[4])\n  B4 <- ((theta[3] - t)*(theta[2]-theta[7]*t)*cosh((theta[3]-t)/(2*theta[4]))^(-2))/(4*theta[4]^2)\n  B5 <- (theta[2] - theta[7]*t)/(2*theta[6]*cosh((theta[5]-t)/theta[6])+2*theta[6])\n  B6 <- ((theta[5] - t)*(theta[7]*t-theta[2])*cosh((theta[5]-t)/(2*theta[6]))^(-2))/(4*theta[6]^2)\n  B7 <- -t*a*((1/(1+exp((theta[3]-t)/theta[4])))-(1/(1+exp((theta[5]-t)/theta[6]))))\n  B <- unname(cbind(B1, B2, B3, B4, B5, B6, B7))\n  \n  return(B)\n}\n\nfit_lm <- function(Y, X, Z,      \n                   delta, sigma2,                     \n                   beta, Omega,                 \n                   mu, Lambda,                      \n                   a, b,                        \n                   niter = 5000){\n  \n  require(Rfast)\n  \n  # indexes\n  p <- dim(delta)[1]\n  q <- dim(Z)[2]\n  N <- dim(delta)[2]\n  n <- sapply(Y, length)\n  \n  # unstack beta\n  B <- matrix(c(beta), nrow=p)\n  Lambda_inv <- diag(1/diag(Lambda))\n  \n  # storage\n  keep_delta  <- array(NA, dim = c(p, N, niter))\n  keep_B      <- array(NA, dim = c(p, q, niter))\n  keep_Omega  <- matrix(NA, nrow = niter, ncol = p)\n  keep_sigma2 <- rep(NA, niter)\n  \n  # set initial values\n  keep_delta[,,1] <- delta\n  keep_B[,,1]     <- B\n  keep_Omega[1,]  <- diag(Omega)\n  keep_sigma2[1]  <- sigma2\n  \n  # pre-computes\n  XtX <- list()\n  XtY <- list()\n  for (k in 1:N){\n    XtX[[k]] <- t(X[[k]])%*%X[[k]]\n    XtY[[k]] <- t(X[[k]])%*%Y[[k]]\n  }\n  ZtZ <- t(Z)%*%Z\n  Lmu <- Lambda_inv%*%mu\n  Ao    <- N/2 + a\n  As    <- sum(n)/2 + a\n  \n  # Gibbs Loop\n  tik <- proc.time()\n  for (iter in 2:niter){\n    \n    Omega_inv <- diag(1/diag(Omega))\n    \n    # sample deltas\n    for (i in 1:N){\n      M         <- (1/sigma2)*XtY[[i]] + Omega_inv%*%(B%*%Z[i,])\n      V_inv     <- chol2inv(chol((1/sigma2)*XtX[[i]] + Omega_inv))\n      delta[,i] <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n    }\n    \n    # sample beta\n    M     <- kronecker(t(Z), Omega_inv)%*%matrix(c(delta), ncol = 1) + Lmu\n    V_inv <- solve(kronecker(ZtZ, Omega_inv) + Lambda_inv)\n    beta  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(q*p)\n    B     <- matrix(beta, nrow = p)\n    \n    # sample omegas\n    for (k in 1:p){\n      Bo <- sum((delta[k,] - (B%*%t(Z))[k,])^2)/2 + b\n      Omega[k,k] <- 1/rgamma(1, Ao, Bo)\n    }\n    \n    # sample sigma2\n    SSE <- 0\n    for (i in 1:N){\n      SSE <- SSE + sum((Y[[i]] - X[[i]]%*%delta[,i])^2)\n    }\n    Bs <- SSE/2 + b\n    sigma2 <- 1/rgamma(1, As, Bs)\n    \n    # store everything\n    keep_delta[,,iter] <- delta\n    keep_B[,,iter]     <- B\n    keep_Omega[iter,]  <- diag(Omega)\n    keep_sigma2[iter]  <- sigma2\n  }\n  tok <- proc.time() - tik\n  \n  return(list(keep_delta = keep_delta, \n              keep_B = keep_B, \n              keep_Omega = keep_Omega, \n              keep_sigma2 = keep_sigma2, \n              comp_time = tok))\n}\n\n\nWe’ll start by simulating some data similar to that in Step 4, then initialize the Gibbs sampler at different values to confirm the chains converge to the same stationary distribution.\n\n\nSimulate the data\n# dimensions\nN <- 40\nm <- 366\nn <- rep(100,N)\np <- 7\nq <- 2\n\ntheta0 <- c(-1.80, 0.75, 120, 8, 270, 8, -7.5)\n\n# Design matrices\ngradinput <- function(x,t){\n  return(double_logis(t,x))\n}\n\n# compute numerical gradient\nXp <- basis_functions(1:366, theta0)\nZ  <- matrix(c(rep(1,N), seq(1:N)), ncol = 2)\n\n# beta parameters\nB0     <- matrix(c(0, 0, -15, 0, 15, 0, 0,\n                   0, 0,   1, 0, -1, 0, 0), nrow=p)\nbeta0  <- matrix(c(B0), ncol = 1)\nOmega0 <- diag(c(0.025, 1e-3,  3, 1,  3, 1, 0.0005))\n\n# delta parameters\ndelta0  <- matrix(0, nrow = p, ncol = N)\nsigma20 <- 0.0025\n\n# sample data\nY <- list()\nX <- list()\nt <- list()\nfor (i in 1:N){\n  # draw delta\n  delta0[,i]   <- t(Rfast::rmvnorm(1, B0%*%Z[i,], Omega0))\n  \n  # draw rows from parent X\n  t[[i]] <- sample(1:m, n[i])\n  X[[i]] <- Xp[t[[i]],]\n  \n  # draw response\n  Y[[i]] <- matrix(rnorm(n[i], mean = X[[i]]%*%delta0[,i], sd = sqrt(sigma20)), ncol=1)\n}\n\n\nFit model with first set of initial values.\n\n\nFit model 1\nniter  <- 5000\ndelta  <- matrix(0, nrow = p, ncol = N)\nbeta   <- rep(0, q*p)\nsigma2 <- 0.01\nOmega  <- diag(c(0.5, 0.5, 5, 0.5, 5, 0.5, 0.001))\n\nmu     <- rep(0, q*p)\nLambda <- diag(rep(1e06,q*p))\na      <- 0.1\nb      <- 0.1\n\nfit1 <- fit_lm(Y, X, Z,\n               delta, sigma2,\n               beta, Omega,\n               mu, Lambda,\n               a, b,\n               niter = niter)\n\n\nFit model with second set of initial values.\n\n\nFit model 2\ndelta  <- matrix(10, nrow = p, ncol = N)\nbeta   <- rep(10, q*p)\nsigma2 <- 2\nOmega  <- 2*diag(p)\n\nmu     <- rep(0, q*p)\nLambda <- diag(rep(1e06,q*p))\na      <- 0.1\nb      <- 0.1\n\nfit2 <- fit_lm(Y, X, Z,\n               delta, sigma2,\n               beta, Omega,\n               mu, Lambda,\n               a, b,\n               niter = niter)\n\n\nFit model with third set of initial values.\n\n\nFit model 3\ndelta  <- matrix(-10, nrow = p, ncol = N)\nbeta   <- rep(-10, q*p)\nsigma2 <- 3\nOmega  <- diag(p)\n\nmu     <- rep(0, q*p)\nLambda <- diag(rep(1e06,q*p))\na      <- 0.1\nb      <- 0.1\n\nfit3 <- fit_lm(Y, X, Z,\n               delta, sigma2,\n               beta, Omega,\n               mu, Lambda,\n               a, b,\n               niter = niter)\n\n\nNext we’ll construct the trace plots of the parameters from each of the three models. Convergence to the same stationary distribution will be apparent if the chains merge together.\nFirst for \\(\\boldsymbol\\beta\\).\n\n\nConstruct trace plots\nwin <- 100:niter\n\nymin <- c(-2, -2, -25, -10, -5, -10, -5)\nymax <- c( 2,  2,   5,  10, 20,  10,  5)\n\npar(mfrow = c(2,2))\n\nfor(l in 1:q){\n  for (k in 1:p){\n    # ymin <- min(fit1$keep_B[k, l, win], fit2$keep_B[k, l, win], fit3$keep_B[k, l, win])\n    # ymax <- max(fit1$keep_B[k, l, win], fit2$keep_B[k, l, win], fit3$keep_B[k, l, win])\n    plot(win, fit1$keep_B[k, l, win], type = \"l\",\n         ylab = bquote(beta[paste(.(l),\",\",.(k))]),\n         xlab = \"iter\",\n         ylim = c(ymin[k], ymax[k]))\n    lines(win, fit2$keep_B[k, l, win], col =\"blue\")\n    lines(win, fit3$keep_B[k, l, win], col =\"green\")\n    abline(h = B0[k, l], col = \"red\", lwd = 2)\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrace plots for \\(\\boldsymbol\\Omega\\).\n\n\nCode\npar(mfrow = c(2,2))\n\nymin <- 0\nymax <- c(0.1, 0.02, 10, 6, 8, 6, 0.3)\n\nfor (k in 1:p){\n  plot(win, fit1$keep_Omega[win, k], type = \"l\",\n       ylab = bquote(omega[paste(.(k),\",\",.(k))]),\n       xlab = \"iter\",\n       ylim = c(ymin, ymax[k]))\n  lines(win, fit2$keep_Omega[win, k], col =\"blue\")\n  lines(win, fit3$keep_Omega[win, k], col =\"green\")\n  abline(h = Omega0[k, k], col = \"red\",lwd = 3)\n}\n\n\n\n\n\n\n\n\nTrace plots for \\(\\sigma^2\\)\n\n\nCode\nplot(win, fit1$keep_sigma2[win], type = \"l\",\n     ylab = bquote(sigma^2),\n     xlab = \"iter\")\nlines(win, fit2$keep_sigma2[win], col =\"blue\")\nlines(win, fit3$keep_sigma2[win], col =\"green\")\nabline(h = sigma20, col = \"red\", lwd = 3)\n\n\n\n\n\nTrace plots for 5 subject \\(\\boldsymbol\\delta_i\\) chosden at random.\n\n\nCode\nwin <- 1:niter\n\npar(mfrow = c(2,2))\nparam_sample <- sample(1:N, 5)\n\nfor (year in param_sample){\n  for (k in 1:p){\n    subscr <- paste0(year,\",\",k)\n    plot(win, fit1$keep_delta[k, year, win], type = \"l\",\n         ylab = bquote(delta[.(subscr)]),\n         xlab = \"iter\")\n    lines(win, fit2$keep_delta[k, year, win], col =\"blue\")\n    lines(win, fit3$keep_delta[k, year, win], col =\"green\")\n    abline(h = delta0[k, year], col = \"red\")\n  }\n}"
  },
  {
    "objectID": "posts/2023-02-02-Step-6-Application to the Double-Logistic Function/index.html",
    "href": "posts/2023-02-02-Step-6-Application to the Double-Logistic Function/index.html",
    "title": "Step 6 - Application to the (orignal) Double-logistic function",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tictoc)\nlibrary(Rfast)\nlibrary(numDeriv)\n\n\nTODO"
  },
  {
    "objectID": "posts/2023-02-02-Step-7-Bayesian spatial CAR model/index.html",
    "href": "posts/2023-02-02-Step-7-Bayesian spatial CAR model/index.html",
    "title": "Step 7 - Areal data and the spatial CAR model - Basic Example",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(viridis)\nlibrary(MASS)\nlibrary(Matrix)\nlibrary(tictoc)\nlibrary(extraDistr)\nlibrary(CARBayes)"
  },
  {
    "objectID": "posts/2023-02-02-Step-7-Bayesian spatial CAR model/index.html#a-brief-introduction",
    "href": "posts/2023-02-02-Step-7-Bayesian spatial CAR model/index.html#a-brief-introduction",
    "title": "Step 7 - Areal data and the spatial CAR model - Basic Example",
    "section": "A Brief Introduction",
    "text": "A Brief Introduction\nHere we’re going to examine the spatial CAR model. CAR stands for Conditionally Autoregressive. The spatial CAR model is in a way an extension of autoregressive models for time series data. Time series data is typically 1-dimensional (in time) and the observations have a natural ordering in the sense that observations can be ordered by the time they were observed. Spatial data can be any dimension and do not necessarily have a natural ordering."
  },
  {
    "objectID": "posts/2023-02-02-Step-7-Bayesian spatial CAR model/index.html#what-is-areal-data",
    "href": "posts/2023-02-02-Step-7-Bayesian spatial CAR model/index.html#what-is-areal-data",
    "title": "Step 7 - Areal data and the spatial CAR model - Basic Example",
    "section": "What is areal data?",
    "text": "What is areal data?\nA CAR model is commonly applied to areal data. That is data where the spatial domain \\(D\\) is partitioned into a finite number of blocks or areas. A common example is the partition of the United States of America into states, census tracts, or counties. A measurement is then collected for each areal unit.\nThe spatial domain is \\(D\\).\nThe areal units are \\(B_i\\) for \\(i = 1,\\dots n\\).\nThe measurements are \\(Z_i \\equiv Z(B_i)\\) for \\(i = 1, \\dots, n\\).\nBefore we dive into the distributional assumptions related to the CAR model, something must be said about the structure of the blocks. Namely we must define some notion of proximity from one block to the next. It’s difficult in general to do this, particularly for an irregular partition of the spatial domain. The easiest approach is to define and adjacency matrix which captures which blocks are bordering other blocks. If there are \\(n\\) blocks, then this adjacency matrix \\(\\mathbf{W}\\) is \\(n \\times n\\) and\n\\[\nw_{ij} =\n\\begin{cases}\n1 \\quad \\text{if } B_i \\text{ shares a border with } B_j,\\\\\n0 \\quad \\text{otherwise}.\n\\end{cases}\n\\] By convention we say that an areal unit does not share a border with itself hence \\(w_{ii} = 0\\) for all \\(i = 1, \\dots, n\\).\nLet’s define our own spatial domain and partition it into some very basic units.\n\n\nCode\nn <- 4\nspat_domain <- expand.grid(x = 1:n, y = 1:n)\nspat_domain$label <- 1:(n*n)\n\n\nHere is the spatial domain. It is a regular lattice with 16 areal units.\n\n\nCode\nggplot(spat_domain) +\n  geom_tile(aes(x, y), linewidth = 2, color = \"grey50\", fill=\"white\") +\n  geom_text(aes(x, y, label=label), size = 15) +\n  coord_fixed() + \n  theme_void()\n\n\n\n\n\nFigure 1: A spatial domain partitioned into a regular lattice with areal units labeled \\(1,...,n\\).\n\n\n\n\nNext we want to define a neighborhood matrix for this regular lattice. A small digression, the convention used to label the areal units will impact the structure of this matrix. Is there a “best” structure? That remains to be seen. For now, let’s stick with the adjacency matrix that arises from the ordering in figure above. Adjacency matrices are abundant in graph theory. We’ll use the package igraph to construct the adjacency matrix for the areal units above. This is accomplished by first using igraph to create a 4 \\(\\times\\) 4 lattice graph, then using the as_adjacency_matrix function to convert the graph object to a sparse matrix.\n\n\nCode\nspat_domain_g <- make_lattice(c(n,n), mutual = TRUE)\nW <- as_adjacency_matrix(spat_domain_g, sparse=1)\nW\n\n\n16 x 16 sparse Matrix of class \"dgCMatrix\"\n                                     \n [1,] . 1 . . 1 . . . . . . . . . . .\n [2,] 1 . 1 . . 1 . . . . . . . . . .\n [3,] . 1 . 1 . . 1 . . . . . . . . .\n [4,] . . 1 . . . . 1 . . . . . . . .\n [5,] 1 . . . . 1 . . 1 . . . . . . .\n [6,] . 1 . . 1 . 1 . . 1 . . . . . .\n [7,] . . 1 . . 1 . 1 . . 1 . . . . .\n [8,] . . . 1 . . 1 . . . . 1 . . . .\n [9,] . . . . 1 . . . . 1 . . 1 . . .\n[10,] . . . . . 1 . . 1 . 1 . . 1 . .\n[11,] . . . . . . 1 . . 1 . 1 . . 1 .\n[12,] . . . . . . . 1 . . 1 . . . . 1\n[13,] . . . . . . . . 1 . . . . 1 . .\n[14,] . . . . . . . . . 1 . . 1 . 1 .\n[15,] . . . . . . . . . . 1 . . 1 . 1\n[16,] . . . . . . . . . . . 1 . . 1 .\n\n\nWith the spatial domain defined and partitioned, we can continue by simulating spatial data. The most basic case assumes spatial independence.Let’s also make it a bit more interesting by bumping up the number of areal units. Now it will probably be a lot easier to spot spatial dependence by a plot of the data alone.\n\n\nCode\nn <- 70\nspat_domain <- expand.grid(x = 1:n, y = 1:n)\nspat_domain$z <- rnorm(n^2, mean = 0, sd = 1)\nggplot(spat_domain) +\n  geom_tile(aes(x, y, fill=z)) +\n  scale_y_reverse() +\n  scale_fill_gradientn(colors = viridis(10)) +\n  coord_fixed() + \n  theme_void()\n\n\n\n\n\nWe want to simulate data with spatial dependence. We can do this from the CAR model.\nLet’s turn to working with the CAR model through a simple example. Let\n\\[\nZ_i =  \\mathbf{x}^T_i\\beta + \\phi_i + \\varepsilon_i\n\\] Here we have a covariate vector \\(\\mathbf{x}_i\\) indexed by spatial location \\(i\\), \\(\\phi_i\\) is a spatial random effect and \\(\\varepsilon_i\\) is a random error associated with the measurement at location \\(i\\) (later we assume to be normal with zero mean and constant variance). The defining characteristic of the CAR model is to specify a spatial structure through the conditional distributions of \\(\\phi_i\\) accordingly\n\\[\n\\phi_i|\\phi_{j, \\; j \\ne i} \\sim \\text{N}\\left(\\textstyle{\\frac{1}{n-1}\\sum}_{j \\ne i} \\phi_j, \\tau^2_i\\right)\n\\] That is the conditional mean of \\(\\phi_i\\) is just the average of the spatial random effects across all other locations. That being said, it’s probably not reasonable to condition on ALL other locations. Paraphrasing Tobler’s First Law of Geography, “everything is related to everything else, but near things are more related than distant things.”\nPerhaps we don’t need to condition on “distant things.” Instead we’ll condition on only the locations we’ve defined as the neighbors of location \\(i\\). Let \\(\\mathcal{N}_i\\) be the set of locations that are considered neighbors with location \\(i\\). Then we specify the conditional distribution as\n\\[\n\\phi_i|\\phi_{j, j \\in \\mathcal{N}_i} \\sim \\text{N}\\left(\\textstyle{\\frac{1}{|\\mathcal{N}_i|}\\sum}_{j \\in \\mathcal{N}_i} \\phi_j, \\tau^2_i\\right)\n\\] Practically, there are too many parameters in this model. Namely we have specified a location-specific variance, \\(\\tau^2_i\\), in each conditional distribution. We can simplify the model by specifying the conditional variance as a function of a parameter shared across locations and the number of neighbors of a given location. This structure is intuitive because we would expect the conditional variance to decrease as the number of neighbors increases. The new conditional distributions are specified as\n\\[\n\\phi_i|\\phi_{j, j \\in \\mathcal{N}_i} \\sim \\text{N}\\left(\\textstyle{\\frac{1}{|\\mathcal{N}_i|}\\sum}_{j \\in \\mathcal{N}_i} \\phi_j, \\frac{\\tau^2}{|\\mathcal{N}_i|}\\right)\n\\] At the end of the day it is possible to write the joint distribution of the spatial random effects from the conditional distributions. This is called compatibility and note that it is not guaranteed! Let \\(\\boldsymbol\\phi = (\\phi_i,\\dots,\\phi_n)\\), \\(\\mathbf{M}\\) be an \\(n \\times n\\) diagonal matrix containing the number of neighbors for each spatial location on its diagonal, and again \\(\\mathbf{W}\\) is our proximity matrix we defined earlier. Finally, we need to introduce another parameter \\(\\rho\\) to ensure that the distribution is proper (details in the BCG 2003).\n\\[\n\\boldsymbol\\phi \\sim \\text{N}\\left(\\boldsymbol{0}, \\tau^2(\\mathbf{M} - \\rho \\mathbf{W})^{-1}\\right)\n\\] The matrix \\(\\mathbf{M}\\) is fairly easy to obtain. Its diagonal is just the row sums from \\(\\mathbf{W}\\) and all other terms set to 0.\nLet’s try to simulate some data from this model. First we’ll define the spatial domain.\n\n\nCode\nn      <- 70\nnsites <- n^2\nspat_domain <- expand.grid(x = 1:n, y = 1:n)\nspat_domain$label <- 1:nsites\nspat_domain_g <- make_lattice(c(n,n), mutual = TRUE)\nW <- as_adjacency_matrix(spat_domain_g, sparse=1)\n\n\nNext define some parameters and draw from the spatial random effects distribution.\n\n\nCode\ntau2 <- 5\nrho <- 0.99\nM   <- diag(rowSums(W))\nspat_prec <- (1/tau2)*(M-rho*W) # swap this with something else. . .\nspat_domain$phi <- backsolve(chol(spat_prec), matrix(rnorm(nsites), nrow = nsites))\n# spat_cov <- tau^{-2}*solve(M-rho*W)\n# \n# phi <- MASS::mvrnorm(1, mu = rep(0,n^2), Sigma = spat_cov)\n\n\nNext sample the observations from the data distribution.\n\n\nCode\nX    <- rep(1, nsites)\nbeta <- matrix(c(2), nrow = 1)\nsigma2 <- 5\nspat_domain$z  <- rnorm(nsites, mean = X%*%beta + spat_domain$phi, sd = sqrt(sigma2))\n\n\nLet’s generate some plots. First, the spatial random effects, then the observations.\n\n\nCode\nggplot(spat_domain) +\n  geom_tile(aes(x, y, fill=phi)) +\n  scale_y_reverse() +\n  scale_fill_gradientn(colors = viridis(10)) +\n  coord_fixed() + \n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(spat_domain) +\n  geom_tile(aes(x, y, fill=z)) +\n  scale_y_reverse() +\n  scale_fill_gradientn(colors = viridis(10)) +\n  coord_fixed() + \n  theme_void()\n\n\n\n\n\n\n\n\n\nSo we’ve simulated some data from the CAR model and it seems fairly clear that the measurements are spatially correlated. Now we want to fit a CAR model to this data and estimate the parameters, \\(\\boldsymbol\\theta = (\\beta, \\sigma^2, \\tau^2, \\rho)\\).\nWe can do this using MCMC. First, let’s summarize the hierarchical model.\n\\[\\begin{align*}\nZ_i &\\sim \\text{N}\\left(x^T_i\\boldsymbol\\beta + \\phi_i, \\; \\sigma^2\\right)\\\\\n\\mu &\\sim \\text{N}\\left(0, \\lambda^2\\right)\\\\\n\\phi_i|\\phi_{j \\in \\mathcal{N}_i} &\\sim \\text{N}\\left(\\frac{\\rho}{m_i}\\sum_{j\\in \\mathcal{N}_i} \\phi_j, \\frac{\\tau^2}{m_i} \\right)\\\\\n\\sigma^2 &\\sim \\text{IG}\\left(a, b\\right)\\\\\n\\tau^2 &\\sim \\text{IG}\\left(a,b\\right)\\\\\n\\rho &\\sim \\text{Unif}\\left(0,1\\right)\n\\end{align*}\\]\nWe’ve stated the model using conditional distributions of \\(\\phi_i\\), though we learned earlier that it is possible to write the joint distribution of \\(\\boldsymbol\\phi\\). If we do this, we will at some point to need invert a very large matrix in order to sample from the full conditional for \\(\\boldsymbol\\phi\\). Instead it might be faster to cycle through the full conditionals of \\(\\phi_i\\) for each \\(i\\).\nMost of this model can be implemented using Gibbs sampling, except when sampling the \\(\\rho\\) parameter. We’ll need to use a Metropolis-Hastings step for that. The full conditionals are as follows,\n\\[\\begin{align*}\n\\boldsymbol\\beta|\\text{rest} &\\sim \\text{N}\\left(\\mathbf{B}^{-1}\\mathbf{A}, \\mathbf{B}^{-1}\\right)\\\\\n\\mathbf{A} &= \\sigma^{-2} \\mathbf{X}^T(Z - \\boldsymbol\\phi)\\\\\n\\mathbf{B} &= \\sigma^{-2}\\mathbf{X}^T\\mathbf{X} + \\lambda^{-2}\\mathbf{I}\\\\\n\\\\\\\\\n\\phi_i|\\text{rest} &\\sim \\text{N}\\left(\\frac{A}{B}, \\frac{1}{B}\\right)\\\\\nA &= \\frac{\\rho}{\\tau^2}\\sum_{j \\in \\mathcal{N}_i}\\phi_j + \\frac{1}{\\sigma^2}(Z_i - x_i^T\\boldsymbol\\beta)\\\\\nB &= \\frac{m_i}{\\tau^2} + \\frac{1}{\\sigma^2}\\\\\n\\\\\\\\\n\\sigma^2|\\text{rest} &\\sim \\text{IG}\\left(A, B \\right)\\\\\nA &= a + \\frac{n}{2}\\\\\nB &= b + \\frac{1}{2}(Z-\\mathbf{X}\\boldsymbol\\beta-\\phi)^T(Z-\\mathbf{X}\\boldsymbol\\beta-\\boldsymbol\\phi)\\\\\n\\\\\\\\\n\\tau^2|\\text{rest} &\\sim \\text{IG} \\left(A, B\\right)\\\\\nA &= a + \\frac{n}{2}\\\\\nB &= b + \\frac{1}{2}\\sum_{i=1}^n m_i\\left(\\phi_i - \\frac{\\rho}{m_i}\\sum_{j \\in \\mathcal{N}_i}\\phi_j\\right)^2\\\\\n\\\\\\\\\np(\\rho|\\text{rest}) &\\propto \\left[\\prod_{i=1}^n p(\\phi_i|\\phi_{j, j\\in\\mathcal{N}_i}, \\rho)\\right]p(\\rho)\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2\\tau^2} \\sum_{i=1}^n m_i \\left(\\phi_i - \\frac{\\rho}{m_i}\\sum_{j \\in \\mathcal{N}_i} \\phi_j \\right)^2\\right\\} \\mathbf{1}\\left\\{\\rho \\in [0,1]\\right\\}\n\\end{align*}\\]\nClearly we are unable to sample directly from the full conditional for \\(\\rho\\). Instead we will need to implement a Metropolis-Hasting step. We will use a truncated normal proposal distribution from the pacakge extraDistr to match the support of \\(\\rho\\). This also let’s us “ignore” the indicator function in the uniform prior because we will never propose a candidate value outside of \\([0,1]\\).\nAs an aside, we will need to routinely compute averages of neighboring spatial random effects at each location. Rather than extract neighbor information from a large neighbor matrix, we define ineighbors as a list of vectors containing the neighbor indices for each location. Additionally we create nneighbors as a vector containing the number of neighbors for each location. These two objects together should give us all we need to efficiently sample full conditionals.\n\n\nCode\nn <- 10\nnsites <- n^2\nspat_domain <- expand.grid(x = 1:n, y = 1:n)\nspat_domain$label <- 1:(n*n)\nspat_domain_g <- make_lattice(c(n,n), mutual = TRUE)\nW <- as_adjacency_matrix(spat_domain_g, sparse=1)\nineighbors <- apply(W, MARGIN = 1, FUN = function(x) which(x==1))\nnneighbors <- rowSums(W)\n\n\nFirst, simulate a small data set.\nNext sample the spatially independent and spatially dependent covariates.\n\n\nCode\n# spatially independent\n# x1 <- rnorm(nsites, mean = 0, sd = 1)\nX <- matrix(1, nrow = nsites)\n# spatial random effect\ntau20 <- 4\nrho0  <- 0.99\nM     <- diag(rowSums(W))\nspat_prec <- (1/tau20)*(M-rho0*W) # swap this with something else. . .\nspat_domain$phi <- backsolve(chol(spat_prec), matrix(rnorm(nsites), nrow = nsites))\nphi0 <- spat_domain$phi\n\n\nNext sample the observations from the data distribution.\n\n\nCode\nbeta0   <- 2\nsigma20 <- 0.25\nspat_domain$z  <- rnorm(nsites, mean = X%*%beta0 + spat_domain$phi, sd = sqrt(sigma20))\nz <- matrix(spat_domain$z, nrow = nsites)\n\n\n\n\nCode\nnpars  <- length(beta)\nnsites <- n^2\nniters <- 5000\nburn   <- 1000\nkeep_beta   <- matrix(NA, nrow = niters, ncol = npars)\nkeep_phi    <- matrix(NA, nrow = niters, ncol = nsites)\nkeep_sigma2 <- rep(NA, niters)\nkeep_tau2   <- rep(NA, niters)\nkeep_rho    <- rep(NA, niters)\n\n# initial values\nbeta   <- keep_beta[1,]  <- 2 #beta0\nphi    <- keep_phi[1,]   <- rep(10, nsites)\nsigma2 <- keep_sigma2[1] <- 2   #sigma20\ntau2   <- keep_tau2[1]   <- 5  # tau20\nrho    <- keep_rho[1]    <- rho0\n\n# prior parameters\na <- 0.1\nb <- 0.1\nlambda2 <- 10000\n\n# Metropolis-Hastings\natt <- 0\nacc <- 0\nMH  <- 0.1\n\n# pre-computes\nXtX <- t(X)%*%X\n\n\n## TODO:\n##    - review rho_loglike funtion.\n# rho_loglike <- function(rho, phi, ineighbors, nneighbors){\n# \n#   sneighbors <- sapply(ineighbors, FUN = function(x) sum(phi[x]))\n#   aneighbors <- sneighbors/nneighbors\n#   \n#   t1 <- sum(nneighbors*phi^2)\n#   t2 <- 2*rho*sum(phi*sneighbors)\n#   t3 <- (rho^2)*sum(nneighbors*(sneighbors^2))\n#   \n#   return(t1 - t2 + t3)\n# }\n\n\ntic()\nfor(iter in 2:niters){\n  \n  # sample mu [Gibbs]\n  A     <- (1/sigma2)*(t(X)%*%(z-phi))\n  B_inv <- solve((1/sigma2)*XtX + (1/lambda2)*diag(npars))\n  beta  <- B_inv%*%A+t(chol(B_inv))%*%rnorm(npars)\n  # beta <- beta0\n  \n  # sample phi [Gibbs]\n  for (site in 1:nsites){\n    A         <- (rho/tau2)*sum(phi[ineighbors[[site]]]) + \n                 (1/sigma2)*(z[site] - X[site,]%*%beta)\n    B_inv     <- 1/(nneighbors[site]/tau2 + 1/sigma2) \n    phi[site] <- rnorm(1, mean = B_inv*A, sd = sqrt(B_inv))\n  }\n  # phi <- phi0\n  \n  # sample sigma2 [Gibbs]\n  A      <- a + (nsites/2)\n  B      <- b + (1/2)*sum((z - X%*%beta - phi)^2)\n  sigma2 <- 1/rgamma(1, A, B)\n  # sigma2 <- sigma20\n  \n  # sample tau2 [Gibbs]\n  aneighbors <- sapply(ineighbors, FUN = function(x) sum(phi[x]))/nneighbors\n  A    <- a + (nsites/2)\n  B    <- b + (1/2)*sum(nneighbors*(phi - rho*aneighbors)^2)\n  tau2 <- 1/rgamma(1, A, B)\n  # tau2 <- tau20\n  \n  ## TODO:\n  ##    - review rho M-H step. Fix rho for now.\n  ## sample rho [M-H]\n  # att <- att + 1 \n  # can <- rtnorm(1, rho, MH, a = 0, b = 1)\n  # R   <- rho_loglike(can, phi, ineighbors, nneighbors) - # Likelihood\n  #        rho_loglike(rho, phi, ineighbors, nneighbors) +\n  #        dtnorm(rho, can , a = 0, b = 1, log = T) -      # M-H adjustment\n  #        dtnorm(can, rho , a = 0, b = 1, log = T)\n  # if(log(runif(1)) < R){\n  #   acc <- acc + 1\n  #   rho <- can\n  # }\n  # rho <- rho0\n  # \n  # # tuning\n  # if(iter < burn){\n  #   if(att > 50){\n  #     if(acc/att < 0.3){MH <- MH*0.8}\n  #     if(acc/att > 0.6){MH <- MH*1.2}\n  #     acc <- att <- 0\n  #   }\n  # }\n  rho <- rho0\n  \n  # storage\n  keep_beta[iter,]  <- beta\n  keep_phi[iter,]   <- phi\n  keep_sigma2[iter] <- sigma2\n  keep_tau2[iter]   <- tau2\n  keep_rho[iter]    <- rho\n  \n}\ntoc()\n\n\n4.44 sec elapsed\n\n\nNow let’s inspect some trace plots.\n\n\nCode\nwin = 1:niters\nplot(win, keep_beta[win,1], type = \"l\")\nabline(h = beta0[1], col = \"red\")\n\n\n\n\n\nCode\nplot(win, keep_sigma2[win], type = \"l\")\nabline(h = sigma20, col = \"red\")\n\n\n\n\n\nCode\nplot(win, keep_tau2[win], type = \"l\")\nabline(h = tau20, col = \"red\")\n\n\n\n\n\nCode\nplot(win, keep_rho[win], type = \"l\")\nabline(h = rho0, col = \"red\")\n\n\n\n\n\nCheck trace plots for \\(\\phi\\). 10 random locations.\n\n\nCode\nsampled_sites <- sample(1:nsites, size = 10)\nfor (site in sampled_sites){\n  plot(win, keep_phi[win, site] + keep_beta[win,1], type = \"l\")\n  abline(h = phi0[site], col = \"red\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere’s clearly something wrong. I’ll need to revisit this and verify the derivations and/or debug the code.\nWhat if we compared to the CARBayes package? Note that CARBayes uses a modified version of the model proposed by Leroux (cite)\n\n\nCode\n# convert neighbord matrix from sparse dgCMatrix format.\nW <- as.matrix(W)\n# fit model from CARBayes.\ncb.model <- S.CARleroux(z~1, family = \"gaussian\", \n                        W=W, burnin = 1000, n.sample = 5000, \n                        rho = rho0, verbose = FALSE)\ncb.model\n\n\n\n#################\n#### Model fitted\n#################\nLikelihood model - Gaussian (identity link function) \nRandom effects model - Leroux CAR\nRegression equation - z ~ 1\nNumber of missing observations - 0\n\n############\n#### Results\n############\nPosterior quantities and DIC\n\n              Mean   2.5%  97.5% n.effective Geweke.diag\n(Intercept) 2.2193 2.0914 2.3335      3099.0         1.6\nnu2         0.2861 0.0041 1.2120        24.8        -3.4\ntau2        3.9512 1.1453 6.1475        28.6         3.6\nrho         0.9900 0.9900 0.9900          NA          NA\n\nDIC =  27.62219       p.d =  -24.97909       LMPL =  -119.43 \n\n\nLet’s look at the CARbayes samples…\n\n\nCode\nplot(cb.model$samples$beta)\n\n\n\n\n\nCode\nplot(cb.model$samples$nu2)\n\n\n\n\n\nCode\nplot(cb.model$samples$tau2)\n\n\n\n\n\nCode\n# plot(cb.model$samples$phi)"
  },
  {
    "objectID": "posts/2023-02-02-Step-8-Simulating-Multivariate-Areal-Data/index.html",
    "href": "posts/2023-02-02-Step-8-Simulating-Multivariate-Areal-Data/index.html",
    "title": "Step 8 - Simulating Multivariate Areal Data",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(viridis)"
  },
  {
    "objectID": "posts/2023-02-02-Step-8-Simulating-Multivariate-Areal-Data/index.html#intro",
    "href": "posts/2023-02-02-Step-8-Simulating-Multivariate-Areal-Data/index.html#intro",
    "title": "Step 8 - Simulating Multivariate Areal Data",
    "section": "Intro",
    "text": "Intro\nWe are going to examine the spatial CAR model in the context of the land surface phenology problem.\nConsider a spatial domain \\(\\mathcal{D}\\) partitioned into a regular lattice of areal units \\(s \\in \\mathcal{D}\\). Then, for a single year, each areal unit will have associated with it a “greenness” curve that is a description of that areal unit’s land surface phenology over the course of the year. The true greenness curve is the object of study. We observe the curve at only a finite number of points according to the capabilities of the collecting satellite. The goal is to recover this curve from these finite points.\nWe model this greenness curve with the so called “double-logistic” function, \\(\\nu(t,\\boldsymbol\\theta)\\), a function of time that is parameterized by the vector \\(\\boldsymbol\\theta = (\\theta_1,\\dots,\\theta_p)\\). Here, \\(p = 7\\). Note: link to previous post mentioning \\(\\nu\\).\nIt is difficult to identify the double-logistic function best characterizing the greenness curve for a single site and single year in isolation due to a paucity of data. Such an approach also neglects the spatial dependence across sites in the domain.\nInstead, we should draw information from other observations that are adjacent in either space or time (on a year-to-year scale).\nFor now, let’s consider modeling the spatial dependence. Fix ourselves to a snapshot in time, a single year, and consider the spatial domain \\(\\mathcal{D}\\) described above. Suppose that in some earlier stage of the analysis, data was aggregated to allow for a estimate to be made for \\(\\boldsymbol\\theta\\) which we will refer to as \\(\\boldsymbol\\theta_{\\mathcal{D}}\\). The result is a “region mean”."
  },
  {
    "objectID": "posts/2023-02-02-Step-8-Simulating-Multivariate-Areal-Data/index.html#a-basic-simulation",
    "href": "posts/2023-02-02-Step-8-Simulating-Multivariate-Areal-Data/index.html#a-basic-simulation",
    "title": "Step 8 - Simulating Multivariate Areal Data",
    "section": "A basic simulation",
    "text": "A basic simulation\nIn this example, we abstract away the data layer of the model and focus on the “deviation parameters”, \\(\\delta_{sj}\\).\nRecall that \\(\\delta_{sj}\\) is a scalar parameter describing the deviation of the \\(j\\)th parameter at location \\(s\\) from the regional average model, \\((\\boldsymbol\\theta_{\\mathcal{D}})_j\\). This is a product of the linearization of the double-logistic function with respect to \\(\\boldsymbol\\theta\\).\nWe will arrange these parameters into vectors in a few different ways. First, let \\(\\boldsymbol\\delta_s\\) be the \\(p \\times 1\\) vector of parameters associated with location \\(s \\in \\mathcal{D}\\). Here \\(\\boldsymbol\\delta_s\\) describes the deviation of the entire phenology curve at location \\(s\\) from the regional average phenology curve. Next, let \\(\\boldsymbol\\delta_{\\cdot j}\\) be the \\(n \\times 1\\) vector of the \\(j\\)th parameter from \\(\\boldsymbol\\delta_s\\) for all \\(s = 1,\\dots,n\\).\nLet the spatial domain be a regular square lattice with \\(n=25\\) areal units.\n\n\nCode\nn      <- 5\nnsites <- n^2\nspat_domain <- expand.grid(x = 1:n, y = 1:n)\nspat_domain$label <- 1:(n*n)\nspat_domain_g <- make_lattice(c(n,n), mutual = TRUE)\nW <- as.matrix(as_adjacency_matrix(spat_domain_g, sparse=1))\nM <- diag(rowSums(W))\n\nggplot(spat_domain) +\n  geom_tile(aes(x, y), linewidth = 2, color = \"grey50\", fill=\"white\") +\n  geom_text(aes(x, y, label=label), size = 15) +\n  coord_fixed() + \n  theme_void()\n\n\n\n\n\nFirst, we sample \\(\\boldsymbol\\delta_{\\cdot j}\\) from a univariate CAR model for each \\(j = 1,\\dots,p\\). Let \\(\\mathbf{W}\\) be the proximity matrix defining the neighbor sites of each site, \\(\\mathbf{M}\\) be a diagonal matrix where the diagonal contains the number of neighboring sites, \\(\\rho\\) be a parameter controlling the strength of spatial association, and \\(\\tau^2_j\\) be [insert interpretation]. Note that \\(\\rho\\) is shared across the \\(j\\) elements and \\(\\tau^2_j\\) is specific to the \\(j\\)th element of \\(\\boldsymbol\\delta_s\\).\n\\[\n\\boldsymbol\\delta_{\\cdot j} \\sim \\text{N}(\\boldsymbol0, \\tau_j^2(\\mathbf{M}-\\rho\\mathbf{W})^{-1})\n\\]\nThis induces spatial dependence independently for each \\(j\\) element of \\(\\boldsymbol\\delta_s\\).\n\n\nCode: univariate CAR samples\np    <- 7\ntau2 <- c(1,1,1,1,1,1,1)\nrho  <- 0.99\nspat_re <- matrix(NA, nrow = nsites, ncol = p)\n\nfor (i in 1:p){\n  spat_prec   <- (1/tau2[i])*(M-rho*W)\n  spat_domain[, ncol(spat_domain) + 1] <- backsolve(chol(spat_prec), \n                                                    matrix(rnorm(nsites), \n                                                           nrow = nsites))\n  colnames(spat_domain)[ncol(spat_domain)] <- paste0(\"phi\", i)\n}\n\nspat_domain <- gather(spat_domain, key = \"phi\", value = \"value\", -c(x,y,label))\n\n\n\n\nCode\nggplot(spat_domain) +\n  geom_tile(aes(x, y, fill=value)) +\n  scale_fill_gradientn(colors = viridis(10)) +\n  coord_fixed() +  \n  theme_void() +\n  facet_wrap(~phi)\n\n\n\n\n\nI’m thinking this might not be a great idea. . . shouldn’t we expect the spatial patterns to be related somehow? Yes, I think so. . , but I believe this will require either the matrix normal distribution or a very large kronecker product."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Research",
    "section": "",
    "text": "Step 0 - Bayesian Linear Regression with Gibbs Sampling\n\n\n\nResearch\n\n\nBayesian\n\n\nMCMC\n\n\n\nStarting from the basics. . .\n\n\n\nMatthew Shisler\n\n\n\n\n\n\n\n\n\n\n\nStep 1 - Bayesian Hierarchical Linear Regression\n\n\n\nBayesian\n\n\nMCMC\n\n\nHierarchical Model\n\n\n\nAdding another level to the linear model.\n\n\n\nMatthew Shisler\n\n\n\n\n\n\n\n\n\n\n\nStep 2 - One Parent Design Matrix\n\n\n\nBayesian\n\n\nMCMC\n\n\nHierarchical Model\n\n\n\nHere we extend to the situation of one parent design matrix from which rows are drawn to construct design matrices for each response vector.\n\n\n\nMatthew Shisler\n\n\n\n\n\n\n\n\n\n\n\nStep 3 - Add a Second Level Linear Trend\n\n\n\nBayesian\n\n\nMCMC\n\n\nHierarchical Model\n\n\n\nIn this extension we add a linear trend to the second level of the hierarchical model and re-derive the full conditionals.\n\n\n\nMatthew Shisler\n\n\n\n\n\n\n\n\n\n\n\nStep 4 - Application to the Linearized Double-Logistic Function\n\n\n\nBayesian\n\n\nMCMC\n\n\nHierarchical Model\n\n\n\nAll previous steps have been building to the application of Gibbs sampling for the double-logistic function.\n\n\n\nMatthew Shisler\n\n\n\n\n\n\n\n\n\n\n\nStep 5 - Verify Convergence\n\n\n\nBayesian\n\n\nMCMC\n\n\nHierarchical Model\n\n\n\nHere we initialize the sampler with different values and check if the chains converge to the same stationary distribution.\n\n\n\nMatthew Shisler\n\n\n\n\n\n\n\n\n\n\n\nStep 6 - Application to the (orignal) Double-logistic function\n\n\n\nBayesian\n\n\nMCMC\n\n\nHierarchical Model\n\n\n\nTODO:\n\n\n\nMatthew Shisler\n\n\n\n\n\n\n\n\n\n\n\nStep 7 - Areal data and the spatial CAR model - Basic Example\n\n\n\nBayesian\n\n\nMCMC\n\n\nSpatial\n\n\nCAR\n\n\n\nA basic description and example of the spatial conditionally autoregressive model for areal data.\n\n\n\nMatthew Shisler\n\n\n\n\n\n\n\n\n\n\n\nStep 8 - Simulating Multivariate Areal Data\n\n\n\nBayesian\n\n\nMCMC\n\n\nSpatial\n\n\nCAR\n\n\n\nWe show how to simulate multivariate areal data with spatial dependence using the CAR model.\n\n\n\nMatthew Shisler\n\n\n\n\n\n\n\n\n\n\n\nST746 - Markov Chain Sim\n\n\n\nStochastic Processes\n\n\n\n\n\n\n\nMatthew Shisler\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-02-03-Markov-Chain-Sim/index.html",
    "href": "posts/2023-02-03-Markov-Chain-Sim/index.html",
    "title": "ST746 - Markov Chain Sim",
    "section": "",
    "text": "Code: Load the packages\nlibrary(tidyverse)\nlibrary(igraph)\nlibrary(viridis)\n\n\nWe want to simulate a simple Markov Chain. The state space is\n\\[\nS = \\{A,B,C\\}\n\\]\nThe transition matrix is\n\\[\n\\begin{pmatrix}\n. & 0.5 & 0.5\\\\\n0.75 & . & 0.25\\\\\n0.75 & . & 0.25\n\\end{pmatrix}\n\\]\nLet \\(X_n\\) be the state of the chain at time \\(n\\). Starting with \\(X_0 = A\\), we will simulate the chain out to \\(X_6\\).\nLet \\(N_B\\) be the number of visits to state \\(B\\) and \\(N_C\\) be the number of visits to state \\(C\\). We would like to approximate the probability of visiting states \\(B\\) and \\(C\\) an equal number of times in the first six steps.\nWith a little work, we can show that it is not possible for \\(N_B = N_C = 0, 1\\). Also, \\(P(N_B = 3, N_B = 3 | X_0 = A) = P(X_1 \\neq A, \\dots, X_6 \\neq A| X_0 = A)\\). The tricky calculation is \\(P(N_B = 2, N_B = 2 | X_0 = A)\\) and that is the motivation for this small simulation.\n\n\nCode: define the simulation function\nmc.sim <- function(P, init.state = 1, num.iters = 50){\n  \n  num.states <- nrow(P)\n  states     <- numeric(num.iters + 1)\n  states[1]  <- 1\n  \n  for (n in 2:(num.iters+1)){\n    \n    p <- P[states[n-1],]\n    states[n] <- which(rmultinom(1,1,p) == 1)\n    \n  }\n  \n  return(chain = states[2:(num.iters+1)])\n  \n}\n\n\nTest the simulation. We should not see any runs of 2 since there is no probability to stay in the same state. We should not see the first state being 1, since we know we must immediately leave state \\(A\\).\n\n\nCode\nP <- matrix(c(0, 0.5, 0.5,\n              0.75,0,0.25,\n              0.75,0.25,0), byrow = T, nrow = 3)\n\nmc.sim(P, init.state = 1, num.iters = 6)\n\n\n[1] 3 1 2 1 3 1\n\n\nThis looks good. We want to run this simulation many times for the first six steps of the chain.\n\n\nCode\nnum.reps <- 10\n\nresults <- matrix(unlist(lapply(1:num.reps,  \n                                function(x) mc.sim(P, \n                                                   init.state = x, \n                                                   num.iters = 6) )), \n                  byrow = T, \n                  nrow = num.reps)\n\n\nHere are the results for 10 replicates:\n\n\nCode\nresults\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6]\n [1,]    2    1    2    1    2    1\n [2,]    3    1    2    1    3    1\n [3,]    2    1    3    1    3    1\n [4,]    2    3    1    2    1    2\n [5,]    3    1    3    1    2    1\n [6,]    3    1    2    1    2    3\n [7,]    2    1    2    1    3    1\n [8,]    2    1    2    3    1    2\n [9,]    3    1    3    1    3    1\n[10,]    2    1    3    2    1    2\n\n\nNext, we want to count each time a row has an equal number of \\(2\\)s and \\(3\\)s corresponding to the number of times we visit states \\(A\\) and \\(B\\) an equal number of times.\n\n\nCode: a function to check for equal visits\ncheck.equal.visit <- function(x, num.times = 2){\n  \n  return(length(which(x==2)) == num.times & length(which(x==3)) == num.times)\n\n}\n\ncheck.equal.visit(results[3,], num.times = 2)\n\n\n[1] FALSE\n\n\nApply this function to the rows of the results matrix.\n\n\nCode\nequal2 <- apply(results, MARGIN = 1, function(x) check.equal.visit(x, num.times = 2))\n\n\nSum this vector to get the number of times the equal visits occured, the dived by the number of simulation replicates to estimate the probability of an equal number of visits.\n\n\nCode\nsum(equal2)/num.reps\n\n\n[1] 0.1\n\n\nRepeat the above, but for a large number of replicates.\n\n\nCode\nnum.reps <- 10000\n\nresults <- matrix(unlist(lapply(1:num.reps,  \n                                function(x) mc.sim(P, \n                                                   init.state = x, \n                                                   num.iters = 6) )), \n                  byrow = T, \n                  nrow = num.reps)\n\nequal0 <- apply(results, MARGIN = 1, function(x) check.equal.visit(x, num.times = 0))\nequal1 <- apply(results, MARGIN = 1, function(x) check.equal.visit(x, num.times = 1))\nequal2 <- apply(results, MARGIN = 1, function(x) check.equal.visit(x, num.times = 2))\nequal3 <- apply(results, MARGIN = 1, function(x) check.equal.visit(x, num.times = 3))\n\nsum(equal0)/num.reps\n\n\n[1] 0\n\n\nCode\nsum(equal1)/num.reps\n\n\n[1] 0\n\n\nCode\nsum(equal2)/num.reps\n\n\n[1] 0.2737\n\n\nCode\nsum(equal3)/num.reps\n\n\n[1] 0.001\n\n\nThis agrees with our estimates, but this is not best way to run the simulation because I don’t need to store the results of each run every time. Let’s write something a little more compact.\nEach time I simulate a replicate I will immediately check for equal visits. Then I can throw away the old results instead of storing them.\n\n\nCode\nnum.reps <- 1000000\n\nnum.equal0 <- 0\nnum.equal1 <- 0\nnum.equal2 <- 0\nnum.equal3 <- 0\n\nfor (i in 1:num.reps){\n  \n  results <- mc.sim(P, init.state = 1, num.iters = 6)\n  \n  if(check.equal.visit(results, num.times = 0)){\n    num.equal0 = num.equal0 + 1 \n  }\n  \n  if(check.equal.visit(results, num.times = 1)){\n    num.equal1 = num.equal1 + 1 \n  }\n  \n  if(check.equal.visit(results, num.times = 2)){\n    num.equal2 = num.equal2 + 1 \n  }\n  \n  if(check.equal.visit(results, num.times = 3)){\n    num.equal3 = num.equal3 + 1 \n  }\n  \n  \n}"
  }
]