{
  "hash": "89b60b98184f07fdcd64c2821adc06af",
  "result": {
    "markdown": "---\ntitle: \"Step 0 - Bayesian Linear Regression with Gibbs Sampling\"\ndescription: \"Starting from the basics. . .\"\nauthor:\n  - name: Matthew Shisler\n    affiliation: North Carloina State University - Department of Statistics\n    affiliation-url: https://statistics.sciences.ncsu.edu/ \ncategories: [Research, Bayesian, MCMC] # self-defined categories\ndraft: false \nformat:\n  html: \n    code-fold: true\n    \nexecute:\n  freeze: auto\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code: Load the packages\"}\nlibrary(tictoc)\n```\n:::\n\n\nWe will start by verifying that our Gibbs sampler is actually working for a very simple case. First, we will simulate data from the model \n\n\\begin{align*}\n\\mathbf{Y} \\sim \\text{Normal}_n(\\mathbf{X}\\mathbf{\\boldsymbol\\delta}, \\; \\sigma^2 \\mathbf{I}_n)\n\\end{align*}\n\nwhere $\\mathbf{Y}$ is an $n \\times 1$ response vector, $\\mathbf{X}$ is an $n \\times p$ design matrix, and $\\boldsymbol\\delta$ is a $p \\times 1$ vector of parameters. The observations are mutually independent with constant variance, $\\text{Cov}(\\mathbf{Y}) = \\sigma^2 \\mathbf{I}_n$. \n\nNext specify priors for $\\boldsymbol\\delta$ and $\\sigma^2$, chosen for conjugacy,\n\\begin{align*}\n\\boldsymbol\\delta &\\sim \\text{Normal}_p(\\boldsymbol\\beta, \\; \\mathbf{\\Omega})\\\\\n\\sigma^2 &\\sim \\text{InvGamma}(a, \\; b)\n\\end{align*}\n\nWhere $\\boldsymbol\\beta = \\boldsymbol0$, $\\mathbf{\\Omega} = \\text{diag}((1000^2, 1000^2))$, and $a = b = 0.1$. The code\nalternates between sampling from the full conditionals, $p(\\boldsymbol\\delta|\\mathbf{Y},\\sigma^2)$ and $p(\\sigma^2|\\mathbf{Y},\\boldsymbol\\delta)$. In this case \n\\begin{align*}\n  \\boldsymbol\\delta|\\mathbf{Y},\\sigma^2 &\\sim \\text{Normal}\\left(V^{-1}M, V^{-1}\\right)\\\\\n  V &= \\frac{1}{\\sigma^{2}}\\mathbf{X}^T\\mathbf{X} + \\mathbf{\\Omega}^{-1}\\\\\n  M &= \\frac{1}{\\sigma^{2}}\\mathbf{X}^T\\mathbf{Y} + \\mathbf{\\Omega}^{-1}\\boldsymbol\\beta\\\\\n  \\sigma^2|\\mathbf{Y},\\boldsymbol\\delta &\\sim \\text{InvGamma}\\left(\\frac{n}{2} + a, \\frac{1}{2}(\\mathbf{Y}-\\mathbf{X}\\boldsymbol\\delta)^T(\\mathbf{Y}-\\mathbf{X}\\boldsymbol\\delta) + b \\right)\n\\end{align*}\n\nLet's do a simple example with $n=100$ and $p = 2$. I will start the parameter index at $1$, i.e. $\\delta_1$ is the intercept.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Simulate some data\"}\nn  <- 100\np  <- 2    #including intercept\n\n# Generate some fake data (with intercept)\nX      <- matrix(c(rep(1,n), rnorm(n*(p-1))), nrow = n, ncol = p)\ndelta0 <- rnorm(p, mean = 0, sd = 3)\nY      <- matrix(rnorm(n, X%*%delta0, sd = 1))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Run the Gibbs sampler\"}\n# set-up\nniter <- 5000\nkeep_delta  <- matrix(NA, nrow = niter, ncol = p)\nkeep_sigma2 <- rep(NA,niter)\n\n# initial values (chosen to be intentionally poor)\ndelta  <- c(-10,10)\nsigma2 <- 10\nkeep_delta[1,] <- delta\nkeep_sigma2[1] <- sigma2\n\n# prior parameters\na  <- 0.1\nb  <- 0.1\nbeta <- rep(0,p)\nOmega_inv <- diag(rep(1e-06,p))\n\n# pre-computes\nXtY <- t(X)%*%Y\nXtX <- t(X)%*%X\nObeta <- Omega_inv%*%beta # not really necessary since beta = 0\n\n# Gibbs Loop\ntic()\nfor (i in 2:niter){\n  \n  # Sample from delta full conditional\n  M     <- (1/sigma2)*XtY + Obeta\n  V_inv <- solve((1/sigma2)*XtX + Omega_inv)\n  delta <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n    \n  # Sample from sigma2 full conditional\n  A      <- n/2 + a\n  B      <- (1/2)*sum((Y-X%*%delta)^2) + b\n  sigma2 <- 1/rgamma(1, A, B)\n  \n  # store the results\n  keep_delta[i,]  <- delta\n  keep_sigma2[i] <- sigma2\n  \n}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.32 sec elapsed\n```\n:::\n:::\n\n\nLet's inspect the resulting trace plots as a quick visual check. The true value of the respective parameter is represented by a red horizontal line. To make the convergence more obvious, I've included the poor initial guess.\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code: Generate trace plots\"}\nwin <- 1:niter\n\npar(mfrow = c(2,2))\n\nplot(win, keep_delta[win,1], type = \"l\",\n     ylab = expression(delta[1]),\n     xlab = \"iter\")\nabline(h = delta0[1], col = \"red\")\n\nplot(win, keep_delta[win,2], type = \"l\",\n     ylab = expression(delta[2]),\n     xlab = \"iter\")\nabline(h = delta0[2], col = \"red\")\n\nplot(win, keep_sigma2[win],   type = \"l\",\n     ylab = expression(sigma^2),\n     xlab = \"iter\")\nabline(h = 1, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWe can \"burn\" the first 100 iterations to see the behavior after convergence. Looks good to me!\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code: Generate trace plots\"}\nwin <- 100:niter\n\npar(mfrow = c(2,2))\n\nplot(win, keep_delta[win,1], type = \"l\",\n     ylab = expression(delta[1]),\n     xlab = \"iter\")\nabline(h = delta0[1], col = \"red\")\n\nplot(win, keep_delta[win,2], type = \"l\",\n     ylab = expression(delta[2]),\n     xlab = \"iter\")\nabline(h = delta0[2], col = \"red\")\n\nplot(win, keep_sigma2[win],   type = \"l\",\n     ylab = expression(sigma^2),\n     xlab = \"iter\")\nabline(h = 1, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}