{
  "hash": "24db39414f92c772c5f276828225a61f",
  "result": {
    "markdown": "---\ntitle: \"Step 4 - Application to the Linearized Double-Logistic Function\"\ndescription: \"All previous steps have been building to the application of Gibbs sampling for the double-logistic function.\"\nauthor:\n  - name: Matthew Shisler\n    affiliation: North Carloina State University - Department of Statistics\n    affiliation-url: https://statistics.sciences.ncsu.edu/ \ncategories: [Bayesian, MCMC, Hierarchical Model] # self-defined categories\ndraft: false \nformat:\n  html: \n    code-fold: true\nexecute: \n  cache: true\n  freeze: auto\n---\n\n::: {.cell hash='index_cache/html/load-packages_20393ab02cca7f43d0ba6312721af7c1'}\n\n```{.r .cell-code  code-summary=\"Code: Load the packages\"}\nlibrary(tictoc)\nlibrary(Rfast)\nlibrary(numDeriv)\n```\n:::\n\n\nLet $Y_{ij}$ be an observation collected on day $t_{ij}$ of year $i$, $i = 1, \\dots, N$ and $j = 1,\\dots,n_i$. Assume $Y_{ij}$ to be normally distributed with mean $v(t_{ij}, \\, \\boldsymbol\\theta_i)$ and constant variance $\\sigma^2$. The mean \"double-logistic\" function, $v(t, \\, \\boldsymbol{\\theta})$, is parameterized by $\\boldsymbol\\theta$ and is defined\n\\begin{equation}\n\\tag{1}\nv(t, \\, \\boldsymbol\\theta) = \\theta_1 + (\\theta_2 - \\theta_7 t)\\left(\\frac{1}{\\exp\\left\\{\\frac{\\theta_3 - t}{\\theta_4}\\right\\}} - \\frac{1}{\\exp\\left\\{\\frac{\\theta_4 - t}{\\theta_6}\\right\\}}\\right)\n\\end{equation}\n\nThere is an additional constraint that $v(t, \\, \\boldsymbol{\\theta}) \\in [0,1]$ and $\\theta_7 \\ge 0$. One option would be to put priors on $\\theta_1$ with $[0,1]$ support and $\\theta_7$ with $[0,\\infty)$ support. One might also consider the constraint $\\theta_2 \\in [\\theta_1, 1]$, but we should be careful to specify a prior whose support depends on another parameter. Instead, we opt not to constrain or transform $\\theta_2$. In general, we would prefer to place a multivariate normal prior over the entire vector $\\boldsymbol\\theta$. To that end, consider a reparameterazation of $v(t,\\boldsymbol\\theta)$,\n\n\\begin{equation}\n\\tag{2}\nv(t, \\, \\boldsymbol\\theta) = \\frac{1}{1+\\exp\\{\\theta_1\\}} + \\left(\\theta_2 - \\exp\\{\\theta_7\\} t\\right)\\left(\\frac{1}{\\exp\\left\\{\\frac{\\theta_3 - t}{\\theta_4}\\right\\}} - \\frac{1}{\\exp\\left\\{\\frac{\\theta_4 - t}{\\theta_6}\\right\\}}\\right)\n\\end{equation}\n\nThe function $v(t, \\, \\boldsymbol{\\theta})$ is a non-linear function of $\\boldsymbol\\theta$ which consequently violates conjugacy of the typically assumed priors. To overcome this we will work with a linearized version of $v(t, \\, \\boldsymbol{\\theta})$. This significantly decreases the computational burden of MCMC at the cost of introducing statistical bias. Are the computational gains worth the induced bias?\n\nThe linearization of $v(t, \\, \\boldsymbol\\theta)$ around $\\boldsymbol\\theta_0$ yields,\n\\begin{equation}\n\\tag{3}\nv(t, \\, \\boldsymbol\\theta) \\approx v(t, \\, \\boldsymbol\\theta_0) + \\nabla_{\\boldsymbol\\theta}v(t, \\, \\boldsymbol\\theta)|_{\\boldsymbol\\theta=\\boldsymbol\\theta_0}(\\boldsymbol\\theta - \\boldsymbol\\theta_0) \n\\end{equation}\n\nDefine the following,\n\n\\begin{equation}\n\\tag{4}\nY^*_{ij} = Y_{ij} - v(t_{ij}, \\, \\boldsymbol\\theta_0)\n\\end{equation}\n\n\\begin{equation}\n\\tag{5}\nX(t) = \\nabla_{\\boldsymbol\\theta}v(t, \\, \\boldsymbol\\theta)|_{\\boldsymbol\\theta=\\boldsymbol\\theta_0}\n\\end{equation}\n\n\\begin{equation}\n\\tag{6}\n\\boldsymbol\\delta_i = \\boldsymbol\\theta_i-\\boldsymbol\\theta_0\n\\end{equation}\n\n\n\\begin{equation}\n\\mathbf{Y}_i \\sim \\text{Normal}_{n_i}\\left(v(\\mathbf{t}_i, \\boldsymbol{\\theta}_i), \\; \\sigma^2 \\mathbf{I}_{n_i}\\right)\n\\end{equation}\n\nConstruct the $m \\times p$ parent design matrix $\\mathbf{X}$ with rows $X(t)$ for $t=1,\\dots,m$. Define the \"child\" design matrix, $\\widetilde{\\mathbf{X}}_i$ to be the subset of rows from $\\mathbf{X}$ specified by $\\mathbf{t}_i$. \n\nThe $N \\times q$ covariate matrix $\\mathbf{Z}$ organizes known covariates for the mean of the distribution for $\\boldsymbol\\delta_i$. Let $\\mathbf{z}_i$ be the $i$th row of $\\mathbf{Z}$ and $\\boldsymbol{\\mathcal{Z}}_i = (\\mathbf{z}^T_i\\otimes \\mathbf{I}_p)$ where \"$\\otimes$\" represents the Kronecker product.\n\nNow we can place a multivariate prior on $\\boldsymbol\\delta_i$ as this parameter represents deviations from $\\boldsymbol\\theta_0$. The linearized model is as follows,\n\n\\begin{align*}\n\\mathbf{Y}^*_i &\\sim \\text{Normal}_{n_i}\\left(\\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i, \\; \\sigma^2 \\mathbf{I}_{n_i}\\right)\\\\\n\\boldsymbol\\delta_i &\\sim \\text{Normal}_p\\left(\\boldsymbol{\\mathcal{Z}}_i\\boldsymbol\\beta, \\; \\mathbf{\\Omega}\\right)\n\\end{align*}\n\nAgain, we will assume $\\mathbf{\\Omega}$ is diagonal and let $\\omega_{kk}$ be the $k$th diagonal element. Next we specify priors,\n\\begin{align*}\n\\boldsymbol\\beta &\\sim \\text{Normal}_{qp}\\left(\\boldsymbol\\mu, \\; \\mathbf{\\Lambda}\\right)\\\\\n\\omega_{kk} &\\sim \\text{InvGamma}\\left(a_\\omega, \\; b_\\omega \\right)\\\\\n\\sigma^2 &\\sim \\text{InvGamma}\\left(a_\\sigma, \\; b_\\sigma\\right)\n\\end{align*}\n\nLet $\\boldsymbol\\Delta = [\\boldsymbol\\delta_1 \\; \\dots \\; \\boldsymbol\\delta_N]$ and $\\mathbf{B} = [\\boldsymbol\\beta_1 \\; \\dots \\; \\boldsymbol\\beta_q]$. That is, $\\mathbf{B}$ is a $p \\times q$ matrix and $\\text{vec}(\\mathbf{B}) = \\boldsymbol\\beta$.  The full conditionals in this model are as follows,\n\n\\begin{align*}\n\\boldsymbol\\delta_i|\\text{rest} &\\sim \\text{Normal}_p(\\mathbf{V}_i^{-1}\\mathbf{M}_i, \\mathbf{V}_i^{-1})\\\\\n\\mathbf{V}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\widetilde{\\mathbf{X}}_i + \\mathbf{\\Omega}^{-1}\\\\\n\\mathbf{M}_i &= \\frac{1}{\\sigma^2} \\widetilde{\\mathbf{X}}_i^T\\mathbf{Y}_i + \\mathbf{\\Omega}^{-1}\\mathbf{B}\\mathbf{z}_i\\\\\\\\\n\\boldsymbol\\beta|\\text{rest} &\\sim \\text{Normal}_{qp}(\\mathbf{V}_\\beta^{-1}\\mathbf{M}_\\beta, \\mathbf{V}_\\beta^{-1}\\\\\n\\mathbf{V}_\\beta &= \\mathbf{Z}^T\\mathbf{Z} \\otimes \\boldsymbol\\Omega^{-1} + \\mathbf{\\Lambda}^{-1}\\\\\n\\mathbf{M}_\\beta &= (\\mathbf{Z}^T \\otimes \\, \\boldsymbol\\Omega^{-1})\\text{vec}(\\boldsymbol\\Delta) + \\mathbf{\\Lambda}^{-1}\\boldsymbol\\mu\\\\\\\\\n\\omega_{kk}|\\text{rest} &\\sim \\text{InvGamma}(A_\\omega,B_\\omega)\\\\\nA_\\omega &= N/2 + a_\\omega\\\\\nB_\\omega &= \\frac{1}{2}\\sum_{i=1}^N (\\delta_{ik} - (\\mathbf{B}\\mathbf{z}_i)_k)^2 + b_\\omega\\\\\\\\\n\\sigma^2|\\text{rest} &\\sim \\text{InvGamma}(A_\\sigma,B_\\sigma)\\\\\nA_\\sigma &= \\frac{1}{2}\\sum_{i=1}^N n_i + a_\\sigma\\\\\nB_\\sigma &= \\frac{1}{2}\\sum_{i=1}^N (\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i)^T(\\mathbf{Y}_i - \\widetilde{\\mathbf{X}}_i\\boldsymbol\\delta_i) + b_\\sigma\n\\end{align*}\n\nNext moving on the code implementation. Define the double logistic function and its gradient.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_a60d416c279cce8f492e6a1db1cbc3ab'}\n\n```{.r .cell-code  code-summary=\"User-defined functions.\"}\n# expit #\n# equivalent to plogis with m=0 & s=1.\nexpit   <- function(x){1/(1+exp(-x))}\n\n# expit_p #\n# first derivative of the expit function. equivalent to dlogis with m=0 & s=1.\nexpit_p <- function(x){expit(x)*(1-expit(x))}\n\n\ndouble_logis <- function(t, theta){\n  # double logistic function.\n  # theta1 is transformed using the logistic function.\n  # theta2 is transformed using the \n  # This allows for all parameters to follow a gaussian distribution\n  \n  theta[1] <- plogis(theta[1])\n  theta[7] <- exp(theta[7])  \n  \n  n1 <- 1\n  d1 <- 1 + exp((theta[3] - t)/theta[4])\n    \n  n2 <- 1\n  d2 <- 1 + exp((theta[5] - t)/theta[6])\n    \n  out <- theta[1] + (theta[2] - theta[7]*t)*(n1/d1 - n2/d2)\n  \n  return(out)\n}\n\n\n# double logistic gradient wrt theta\nbasis_functions <- function(t,theta){\n  t   <- t%%366\n\n  dl0 <- double_logis(t,theta)-expit(theta[1])\n  a <- expit_p(theta[7])\n  theta[7] <- exp(theta[7])\n\n  B1 <- expit_p(theta[1])\n  B2 <- (1/(1+exp((theta[3]-t)/theta[4])))-(1/(1+exp((theta[5]-t)/theta[6])))\n  B3 <- (theta[7]*t - theta[2])/(2*theta[4]*cosh((theta[3]-t)/theta[4])+2*theta[4])\n  B4 <- ((theta[3] - t)*(theta[2]-theta[7]*t)*cosh((theta[3]-t)/(2*theta[4]))^(-2))/(4*theta[4]^2)\n  B5 <- (theta[2] - theta[7]*t)/(2*theta[6]*cosh((theta[5]-t)/theta[6])+2*theta[6])\n  B6 <- ((theta[5] - t)*(theta[7]*t-theta[2])*cosh((theta[5]-t)/(2*theta[6]))^(-2))/(4*theta[6]^2)\n  B7 <- -t*a*((1/(1+exp((theta[3]-t)/theta[4])))-(1/(1+exp((theta[5]-t)/theta[6]))))\n  B <- unname(cbind(B1, B2, B3, B4, B5, B6, B7))\n\n  return(B)\n}\n```\n:::\n\n\nWe need to specify a value of $\\boldsymbol\\theta_0$ on which to center the linearization. We will expand on how to\nobtain this value at a later time.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_e93767ce4c591664e2992b6b0affecd3'}\n\n```{.r .cell-code}\ntheta0 <- c(-1.80, 0.75, 120, 8, 270, 8, -7.5)\n\nplot(1:366, double_logis(1:366, theta0), type = \"l\",\n     xlab = \"t\",\n     ylab = bquote(v(t,theta[0])))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWe verify that the analytical gradient is correct by checking the result against a numerical gradient function.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_9c4837cb85f95f08fe8519317e658e25'}\n\n```{.r .cell-code  code-summary=\"Construct the numerical and analytical gradiant matrix\"}\ntheta0 <- c(-1.80, 0.75, 120, 8, 270, 8, -7.5)\n\ngradinput <- function(x,t){\n  return(double_logis(t,x))\n}\n\n# compute numerical gradient\nG <- matrix(0, nrow = 366, ncol = 7)\nfor (i in 1:366){\n  G[i,] <- grad(gradinput, x = theta0, t=i)\n}\n\n# # compute the analytical gradient\nX <- basis_functions(1:366, theta0)\n\nmatplot(G, type=\"l\", main = \"Numerical Gradient\", xlab = \"t\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code  code-summary=\"Construct the numerical and analytical gradiant matrix\"}\nmatplot(X, type=\"l\", main = \"Analytical Gradient\", xlab = \"t\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n\nNext we simulate some data from the linearized model. (Need to add details regarding simulation set-up).\n\n::: {.cell hash='index_cache/html/unnamed-chunk-5_c48d1fec7b6cfae115ec56f09fa8d0d5'}\n\n```{.r .cell-code  code-summary=\"Simulate the data\"}\n# dimensions\nN <- 40\nm <- 366\nn <- rep(100,N)\np <- 7\nq <- 2\n\ntheta0 <- c(-1.80, 0.75, 120, 8, 270, 8, -7.5)\n\n# Design matrices\ngradinput <- function(x,t){\n  return(double_logis(t,x))\n}\n\n# compute numerical gradient\nXp <- basis_functions(1:366, theta0)\nZ  <- matrix(c(rep(1,N), seq(1:N)), ncol = 2)\n\n# beta parameters\nB0     <- matrix(c(0, 0, -15, 0, 15, 0, 0,\n                   0, 0,   1, 0, -1, 0, 0), nrow=p)\nbeta0  <- matrix(c(B0), ncol = 1)\nOmega0 <- diag(c(0.025, 1e-3,  3, 1,  3, 1, 0.0005))\n\n# delta parameters\ndelta0  <- matrix(0, nrow = p, ncol = N)\nsigma20 <- 0.0025\n\n# sample data\nY <- list()\nX <- list()\nt <- list()\nfor (i in 1:N){\n  # draw delta\n  delta0[,i]   <- t(Rfast::rmvnorm(1, B0%*%Z[i,], Omega0))\n  \n  # draw rows from parent X\n  t[[i]] <- sample(1:m, n[i])\n  X[[i]] <- Xp[t[[i]],]\n  \n  # draw response\n  Y[[i]] <- matrix(rnorm(n[i], mean = X[[i]]%*%delta0[,i], sd = sqrt(sigma20)), ncol=1)\n}\n```\n:::\n\n\nPlot the simulated curves. We won't plot the individual observations.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-6_8cd635533d001c158682260658045021'}\n\n```{.r .cell-code}\nplot(1:366, double_logis(1:366, theta0), type = \"l\", ylim=c(-0.1, 1.2))\n\nfor (i in 1:N){\n  lines(1:366, double_logis(1:366, theta0) + Xp%*%delta0[,i], type=\"l\", col = i,\n        xlab = \"t\",\n        ylab = \"Y\",\n        main = \"Simulated Curves\")\n  # points(t[[i]], Y[[i]] + double_logis(t[[i]], theta0), pch=16)\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThe Gibbs loop. Later we will re-write this as a function.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-7_9f42156cedb905bc9a468e5d6b4c99f3'}\n\n```{.r .cell-code  code-summary=\"Run the Gibbs sampler\"}\n# set-up\nniter <- 5000\nkeep_delta  <- array(NA, dim = c(p, N, niter))\nkeep_B      <- array(NA, dim = c(p, q, niter))\nkeep_Omega  <- matrix(NA, nrow = niter, ncol = p)\nkeep_sigma2 <- rep(NA, niter)\n\n# initial values\ndelta  <- matrix(0, nrow = p, ncol = N)\nbeta   <- rep(0, q*p)\nB      <- matrix(c(beta), nrow=p)\nsigma2 <- 0.01\nOmega  <- diag(c(0.5, 0.5, 5, 0.5, 5, 0.5, 0.001))\nkeep_delta[,,1] <- delta\nkeep_B[,,1]     <- B\nkeep_Omega[1,]  <- diag(Omega)\nkeep_sigma2[1]  <- sigma2\n\n# prior parameters\nmu    <- rep(0, q*p)\nLambda_inv <- diag(rep(1e-06,q*p))\na     <- 0.1\nb     <- 0.1\n\n# pre-computes\nXtX <- list()\nXtY <- list()\nfor (k in 1:N){\n  XtX[[k]] <- t(X[[k]])%*%X[[k]]\n  XtY[[k]] <- t(X[[k]])%*%Y[[k]]\n}\nZtZ <- t(Z)%*%Z\nLmu <- Lambda_inv%*%mu\nAo    <- N/2 + a\nAs    <- sum(n)/2 + a\n\n# Gibbs Loop\ntic()\nfor (iter in 2:niter){\n\n  Omega_inv <- diag(1/diag(Omega))\n  \n  # sample deltas\n  for (i in 1:N){\n    M         <- (1/sigma2)*XtY[[i]] + Omega_inv%*%(B%*%Z[i,])\n    V_inv     <- chol2inv(chol((1/sigma2)*XtX[[i]] + Omega_inv))\n    delta[,i] <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  }\n  \n  # sample beta\n  M     <- kronecker(t(Z), Omega_inv)%*%matrix(c(delta), ncol = 1) + Lmu\n  V_inv <- solve(kronecker(ZtZ, Omega_inv) + Lambda_inv)\n  beta  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(q*p)\n  B     <- matrix(beta, nrow = p)\n  \n  # sample omegas\n  for (k in 1:p){\n    Bo <- sum((delta[k,] - (B%*%t(Z))[k,])^2)/2 + b\n    Omega[k,k] <- 1/rgamma(1, Ao, Bo)\n  }\n  \n  # sample sigma2\n  SSE <- 0\n  for (i in 1:N){\n    SSE <- SSE + sum((Y[[i]] - X[[i]]%*%delta[,i])^2)\n  }\n  Bs <- SSE/2 + b\n  sigma2 <- 1/rgamma(1, As, Bs)\n  \n  # store everything\n  keep_delta[,,iter] <- delta\n  keep_B[,,iter]     <- B\n  keep_Omega[iter,]  <- diag(Omega)\n  keep_sigma2[iter]  <- sigma2\n}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n11.15 sec elapsed\n```\n:::\n:::\n\n\nNow for some trace plots. The following are for $\\mathbf{B}$, $\\boldsymbol\\Omega$, and $\\sigma^2$. And display iterations 100:5000. This visual inspection seems to indicate good convergence!\n\n::: {.cell hash='index_cache/html/unnamed-chunk-8_c2d03a471706a4591825aef4cf46f57c'}\n\n```{.r .cell-code  code-summary=\"Construct trace plots\"}\nwin <- 1:niter\n\npar(mfrow = c(2,2))\n\nfor(l in 1:q){\n  for (k in 1:p){\n    plot(win, keep_B[k, l, win], type = \"l\",\n         ylab = bquote(beta[paste(.(l),\",\",.(k))]),\n         xlab = \"iter\")\n    abline(h = B0[k, l], col = \"red\")\n  }\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n\n```{.r .cell-code  code-summary=\"Construct trace plots\"}\nfor (k in 1:p){\n  plot(win, keep_Omega[win, k], type = \"l\",\n       ylab = bquote(omega[paste(.(k),\",\",.(k))]),\n       xlab = \"iter\")\n  abline(h = Omega0[k, k], col = \"red\")\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-5.png){width=672}\n:::\n\n```{.r .cell-code  code-summary=\"Construct trace plots\"}\nplot(win, keep_sigma2[win], type = \"l\",\n     ylab = bquote(sigma^2),\n     xlab = \"iter\")\nabline(h = sigma20, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-6.png){width=672}\n:::\n:::\n\n\nTrace plots for 5 subject $\\boldsymbol\\delta_i$ chosen at random.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-9_69966274af95357041a7ae3d13dcd122'}\n\n```{.r .cell-code  code-summary=\"Construct trace plots\"}\nwin <- 1:niter\n\npar(mfrow = c(2,2))\nparam_sample <- sample(1:N, 5)\n\nfor (year in param_sample){\n  for (k in 1:p){\n    subscr <- paste0(year,\",\",k)\n    plot(win, keep_delta[k, year, win], type = \"l\",\n         ylab = bquote(delta[.(subscr)]),\n         xlab = \"iter\")\n    abline(h = delta0[k, year], col = \"red\")\n  }\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-9.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}