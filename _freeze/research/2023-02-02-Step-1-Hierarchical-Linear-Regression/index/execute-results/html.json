{
  "hash": "53797d0942ee907e1dd90632ae390447",
  "result": {
    "markdown": "---\ntitle: \"Step 1 - Bayesian Hierarchical Linear Regression\"\ndescription: \"Adding another level to the linear model.\"\nauthor:\n  - name: Matthew Shisler\n    affiliation: North Carloina State University - Department of Statistics\n    affiliation-url: https://statistics.sciences.ncsu.edu/ \ncategories: [Bayesian, MCMC, Hierarchical Model] # self-defined categories\ndraft: false \nformat:\n  html: \n    code-fold: true\nexecute: \n  cache: true\n  freeze: auto\n---\n\n::: {.cell hash='index_cache/html/load-packages_33d89cbae04dd9557210b6bf96e1061f'}\n\n```{.r .cell-code  code-summary=\"Code: Load the packages\"}\nlibrary(tictoc)\nlibrary(Rfast)\n```\n:::\n\n\nConsider an extension of the setting from Step 1 where $\\mathbf{Y}_i$ is an $n_i \\times 1$ response vector , $\\mathbf{X}_i$ an $n_i \\times p$ design matrix, and $\\boldsymbol\\delta_i$ a $p \\times 1$ parameter vector corresponding to subject $i = 1,\\dots,N$ (where the subject will later be the year). I did not index year using \"$t$\" because later we will define $t_{ij}$ to be the day of year $i$ on which the $j$th measurement of $\\mathbf{Y}_i$ was collected.\n\nThe parameters $\\boldsymbol\\delta_i$ are drawn from a multivariate normal random effects distribution with mean $\\beta$ and covariance matrix $\\mathbf{\\Omega}$. The entries of $\\mathbf{Y}_i$ are mutually independent with constant variance $\\sigma^2$, $\\text{Cov}(\\mathbf{Y}_i) = \\sigma^2 \\mathbf{I}_n$ for all $i$. Further, $\\mathbf{Y}_1,\\dots,\\mathbf{Y}_N$ are mutually independent.\n\n\\begin{align*}\n\\mathbf{Y}_i &\\sim \\text{Normal}_{n_i}\\left(\\mathbf{X}_i\\boldsymbol\\delta_i, \\; \\sigma^2 \\mathbf{I}_{n_i}\\right)\\\\\n\\boldsymbol\\delta_i &\\sim \\text{Normal}_p\\left(\\boldsymbol\\beta, \\; \\mathbf{\\Omega}\\right)\n\\end{align*}\n\nIn this case we will assume $\\mathbf{\\Omega}$ is diagonal and let $\\omega_{kk}$ be the $k$th diagonal element. Next specify priors,\n\\begin{align*}\n\\boldsymbol\\beta &\\sim \\text{Normal}_p\\left(\\boldsymbol\\mu, \\; \\mathbf{\\Lambda}\\right)\\\\\n\\omega_{kk} &\\sim \\text{InvGamma}\\left(a_{k}, \\; b_{k} \\right)\\\\\n\\sigma^2 &\\sim \\text{InvGamma}\\left(a, \\; b\\right)\n\\end{align*}\n\nNote, for simplicity there is no linear trend in the random effects distribution for $\\boldsymbol\\delta_i$. Also, I've left its covariance to be diagonal, just to avoid the Inverse Wishart prior for now. This way we can update the diagonal elements of $\\mathbf{\\Omega}$ individually.\n\nThe full conditionals in this model are. . . \n\n\\begin{align*}\n\\boldsymbol\\delta_i|\\text{``rest\"} &\\sim \\text{Normal}_p(\\mathbf{V}_i^{-1}\\mathbf{M}_i, \\mathbf{V}_i^{-1})\\\\\n\\mathbf{V}_i &= \\frac{1}{\\sigma^2} \\mathbf{X}_i^T\\mathbf{X}_i + \\mathbf{\\Omega}^{-1}\\\\\n\\mathbf{M}_i &= \\frac{1}{\\sigma^2} \\mathbf{X}_i^T\\mathbf{Y}_i + \\mathbf{\\Omega}^{-1}\\boldsymbol\\beta\\\\\\\\\n\\boldsymbol\\beta|\\text{``rest\"} &\\sim \\text{Normal}_p(\\mathbf{V}_\\beta^{-1}\\mathbf{M}_\\beta, \\mathbf{V}_\\beta^{-1})\\\\\n\\mathbf{V}_\\beta &= N\\mathbf{\\Omega}^{-1} + \\mathbf{\\Lambda}^{-1}\\\\\n\\mathbf{M}_\\beta &= \\mathbf{\\Omega}^{-1}\\sum_{i=1}^N\\boldsymbol\\delta_i + \\mathbf{\\Lambda}^{-1}\\boldsymbol\\mu\\\\\\\\\n\\omega_{kk}|\\text{``rest\"} &\\sim \\text{InvGamma}(A_k,B_k)\\\\\nA_k &= N/2 + a_k\\\\\nB_k &= \\frac{1}{2}\\sum_{i=1}^N (\\delta_{ik} - \\beta_k)^2 + b_k\\\\\\\\\n\\sigma^2|\\text{\"rest\"} &\\sim \\text{InvGamma}(A,B)\\\\\nA &= \\frac{1}{2}\\sum_{i=1}^N n_i + a\\\\\nB &= \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^{n_i} (Y_{ij} - \\mathbf{X}_i\\boldsymbol\\delta_i)^2 + b\n\\end{align*}\n\nSimulate some data from this model. In this case will we set $N = 100$, $p = 2$, and $n_i = n = 100$. Then we will implement the Gibbs sampler.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_3f8ab62f3cf619611fcb7cdf41dfd4c1'}\n\n```{.r .cell-code  code-summary=\"Simulate some data\"}\nN <- 100\np <- 2\nn <- rep(100, N)\n\nbeta0  <- rnorm(p, mean = 0, sd = 5)\nOmega0 <- diag(c(2,1))\n\ndelta0  <- t(Rfast::rmvnorm(N, beta0, Omega0))\nsigma20 <- 1\n\nY <- list()\nX <- list()\nfor (i in 1:N){\n  X[[i]] <- matrix(c(rep(1,n[i]), rnorm(n[i]*(p-1))), nrow = n[i], ncol = p)\n  Y[[i]] <- matrix(rnorm(n[i], mean = X[[i]]%*%delta0[,i], sd = sqrt(sigma20)), ncol=1)\n}\n```\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_0c2f7dd6651e11c41070bc540ec47566'}\n\n```{.r .cell-code  code-summary=\"Run the Gibbs sampler\"}\n# set-up\nniter <- 5000\nkeep_delta  <- array(NA, dim = c(p, N, niter))\nkeep_beta   <- matrix(NA, nrow = niter, ncol = p)\nkeep_Omega  <- matrix(NA, nrow = niter, ncol = p)\nkeep_sigma2 <- rep(NA, niter)\n\n# initial values\ndelta  <- matrix(0, nrow = p, ncol = N)\nbeta   <- rep(10, p)\nsigma2 <- 3\nOmega  <- diag(c(5,5))\nkeep_delta[,,1] <- delta\nkeep_beta[1,]   <- beta\nkeep_Omega[1,]  <- diag(Omega)\nkeep_sigma2[1]  <- sigma2\n\n\n# prior parameters\nmu    <- rep(0, p)\nLambda_inv <- diag(rep(1e-06,p))\na     <- 0.1\nb     <- 0.1\nAo    <- N/2 + a\nAs    <- sum(n)/2 + a\n\n# pre-computes\nXtX <- list()\nXtY <- list()\nfor (k in 1:N){\n  XtX[[k]] <- t(X[[k]])%*%X[[k]]\n  XtY[[k]] <- t(X[[k]])%*%Y[[k]]\n}\nLmu <- Lambda_inv%*%mu\n\ntic()\n# Gibbs Loop\nfor (i in 2:niter){\n  \n  Omega_inv <- diag(1/diag(Omega))\n  \n  # sample deltas\n  for (k in 1:N){\n    M         <- (1/sigma2)*XtY[[k]] + Omega_inv%*%beta\n    V_inv     <- chol2inv(chol((1/sigma2)*XtX[[k]] + Omega_inv))\n    delta[,k] <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  }\n  \n  # sample beta\n  M     <- Omega_inv%*%rowSums(delta0) + Lmu\n  V_inv <- solve(N*Omega_inv + Lambda_inv)\n  beta  <- V_inv%*%M+t(chol(V_inv))%*%rnorm(p)\n  \n  # sample omegas\n  for (j in 1:p){\n    Bo <- sum((delta[j,] - beta[j])^2)/2 + b\n    Omega[j,j] <- 1/rgamma(1, Ao, Bo)\n  }\n  \n  # sample sigma2\n  SSE <- 0\n  for (k in 1:N){\n    SSE <- SSE + sum((Y[[k]] - X[[k]]%*%delta[,k])^2)\n  }\n  Bs <- SSE/2 + b\n  sigma2 <- 1/rgamma(1, As, Bs)\n  \n  # store everything\n  keep_delta[,,i] <- delta\n  keep_beta[i,]   <- beta\n  keep_Omega[i,]  <- diag(Omega)\n  keep_sigma2[i]  <- sigma2\n}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n24.1 sec elapsed\n```\n:::\n:::\n\nThis sampler takes about 20 seconds to run on my machine. That seems slow relative to JAGS. I'm sure there are some computational tricks that I can employ. The way I am computing the overall SSE to update $\\sigma^2$ seems particularly naive.\n\nInspect some trace plots. The true value of the respective parameter is represented by a red horizontal line. Again, I've included the poor initial guess to make the convergence more obvious.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_b1bc22d71b60fa9ff1795ed90b923541'}\n\n```{.r .cell-code  code-summary=\"Construct trace plots\"}\nwin <- 1:niter\n\npar(mfrow = c(2,2))\n\nfor (k in 1:p){\n  plot(win, keep_beta[win,k], type = \"l\",\n       ylab = bquote(beta[.(k)]),\n       xlab = \"iter\")\n  abline(h = beta0[k], col = \"red\")\n}\n\nfor (k in 1:p){\n  plot(win, keep_Omega[win, k],   type = \"l\",\n       ylab = bquote(omega[.(k+10*k)]),\n       xlab = \"iter\")\n  abline(h = Omega[k,k], col = \"red\")\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code  code-summary=\"Construct trace plots\"}\n# par(mfrow = c(1,1))\n\nplot(win, keep_sigma2[win],   type = \"l\",\n     ylab = bquote(sigma^2),\n     xlab = \"iter\")\nabline(h = sigma20, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}